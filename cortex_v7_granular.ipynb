{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Cortex-7: Granular Research Lab\n",
                "\n",
                "Bienvenido a la versi贸n modular de Cortex. Hemos fragmentado el c贸digo en **20 pasos ejecutables** para que puedas inspeccionar cada tornillo de la m谩quina.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Librer铆as y Setup Inicial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random\n",
                "import requests\n",
                "from IPython.display import clear_output, display\n",
                "\n",
                "# Configuraci贸n del dispositivo\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\" Estamos usando: {device.upper()}\")\n",
                "\n",
                "# Para reproducibilidad (Semilla de Dios)\n",
                "def set_seed(seed=1337):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
                "    print(f\" Semilla fijada en: {seed}\")\n",
                "\n",
                "set_seed(1337)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Carga de Datos (Shakespeare)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_shakespeare():\n",
                "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
                "    print(\" Descargando Shakespeare...\")\n",
                "    return requests.get(url).text\n",
                "\n",
                "text = get_shakespeare()\n",
                "print(f\"Longitud del dataset: {len(text)} caracteres\")\n",
                "print(\"\\n--- Muestra ---\")\n",
                "print(text[:200])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Tokenizaci贸n (Byte-Level)\n",
                "En lugar de un vocabulario complejo, usamos los 256 bytes crudos. Esto hace al modelo universal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mapeo directo de caracteres a enteros (ASCII/UTF-8)\n",
                "chars = sorted(list(set(text)))\n",
                "vocab_size = 256 # Forzamos 256 para ser Byte-Level real, aunque Shakespeare use menos\n",
                "\n",
                "stoi = { ch:ord(ch) for ch in chars }\n",
                "itos = { i:chr(i) for i in range(256) }\n",
                "\n",
                "encode = lambda s: [ord(c) for c in s]\n",
                "decode = lambda l: ''.join([chr(i) for i in l])\n",
                "\n",
                "print(f\"Tama帽o del Vocabulario: {vocab_size} (Bytes)\")\n",
                "test_str = \"Hello Cortex\"\n",
                "print(f\"Prueba: '{test_str}' -> {encode(test_str)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Preparaci贸n de Batches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_tensor = torch.tensor(encode(text), dtype=torch.long)\n",
                "n = int(0.9 * len(data_tensor))\n",
                "train_data = data_tensor[:n]\n",
                "val_data = data_tensor[n:]\n",
                "\n",
                "def get_batch(split='train', batch_size=32, block_size=64):\n",
                "    data = train_data if split == 'train' else val_data\n",
                "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
                "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
                "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
                "    return x, y\n",
                "\n",
                "xb, yb = get_batch()\n",
                "print(f\"Batch Shape: {xb.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Componente A: Mamba Block (Memoria)\n",
                "Este bloque reemplaza a la atenci贸n para memoria de largo plazo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MambaBlock(nn.Module):\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_model, d_model * 2)\n",
                "        self.out_proj = nn.Linear(d_model, d_model)\n",
                "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        B, L, D = x.shape\n",
                "        x_and_res = self.in_proj(x)\n",
                "        x_val, res = x_and_res.chunk(2, dim=-1)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = self.conv(x_val)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = F.silu(x_val)\n",
                "        return self.out_proj(x_val * F.sigmoid(res))\n",
                "\n",
                "# --- PRUEBA UNITARIA MAMBA ---\n",
                "dummy_x = torch.randn(1, 64, 128).to(device)\n",
                "mamba = MambaBlock(128).to(device)\n",
                "out = mamba(dummy_x)\n",
                "print(f\"Mamba Test: Input {dummy_x.shape} -> Output {out.shape} (Debe ser igual)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Componente B: Attention (Razonamiento)\n",
                "Usamos atenci贸n est谩ndar pero instrumentada para ver los mapas de calor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class InstrumentedAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_head):\n",
                "        super().__init__()\n",
                "        self.mha = nn.MultiheadAttention(d_model, n_head, batch_first=True)\n",
                "        self.last_weights = None\n",
                "        \n",
                "    def forward(self, x):\n",
                "        out, weights = self.mha(x, x, x, need_weights=True, average_attn_weights=True)\n",
                "        self.last_weights = weights.detach().cpu()\n",
                "        return out\n",
                "\n",
                "# --- PRUEBA UNITARIA ATENCIN ---\n",
                "attn = InstrumentedAttention(128, 4).to(device)\n",
                "out = attn(dummy_x)\n",
                "print(f\"Attention Test: Output {out.shape}\")\n",
                "\n",
                "# Visualizar Heatmap Dummy\n",
                "plt.figure(figsize=(4, 3))\n",
                "sns.heatmap(attn.last_weights[0].numpy(), cmap='viridis')\n",
                "plt.title(\"Prueba de Heatmap (Ruido)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. El Organismo (Cortex)\n",
                "Ensamblamos las piezas. Incluimos telemetr铆a para el \"Glass Box\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CortexOrganism(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.embedding = nn.Embedding(256, config['d_model'])\n",
                "        self.layers = nn.ModuleList()\n",
                "        self.activations = {}\n",
                "        self.residual_velocities = []\n",
                "\n",
                "        for i in range(config['n_layers']): \n",
                "            # L贸gica H铆brida: Pares = Mamba, Impares = Atenci贸n\n",
                "            if i % 2 == 0: \n",
                "                self.layers.append(MambaBlock(config['d_model']))\n",
                "            else: \n",
                "                self.layers.append(InstrumentedAttention(config['d_model'], config['n_heads']))\n",
                "                \n",
                "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
                "        self.head = nn.Linear(config['d_model'], 256)\n",
                "\n",
                "    def forward(self, idx, targets=None):\n",
                "        self.activations = {}\n",
                "        self.residual_velocities = []\n",
                "        x = self.embedding(idx)\n",
                "        \n",
                "        for i, layer in enumerate(self.layers):\n",
                "            prev_x = x\n",
                "            x = layer(x)\n",
                "            # Telemetr铆a\n",
                "            with torch.no_grad():\n",
                "                self.activations[f\"layer_{i}\"] = x.detach().cpu()\n",
                "                self.residual_velocities.append((x - prev_x).norm().item())\n",
                "                \n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        \n",
                "        loss = None\n",
                "        if targets is not None:\n",
                "            B, T, C = logits.shape\n",
                "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
                "        return logits, loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Generador de Texto\n",
                "Funci贸n auxiliar para ver qu茅 dice el modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate(model, prompt, max_len=50):\n",
                "    model.eval()\n",
                "    idx = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
                "    for _ in range(max_len):\n",
                "        with torch.no_grad():\n",
                "            logits, _ = model(idx)\n",
                "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
                "            next_token = torch.multinomial(probs, 1)\n",
                "            idx = torch.cat((idx, next_token), dim=1)\n",
                "    return decode(idx[0].tolist())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. PRUEBA DE FUEGO: Sanity Check\n",
                "Entrenamos con UNA sola frase. Si falla aqu铆, no seguimos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\" Iniciando Sanity Check...\")\n",
                "sanity_text = \"To be or not to be\"\n",
                "data = torch.tensor(encode(sanity_text), dtype=torch.long).unsqueeze(0).to(device)\n",
                "x_sanity, y_sanity = data[:, :-1], data[:, 1:]\n",
                "\n",
                "model = CortexOrganism({'n_layers': 2, 'd_model': 128, 'n_heads': 4}).to(device)\n",
                "optim = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
                "\n",
                "losses = []\n",
                "for i in range(100):\n",
                "    _, loss = model(x_sanity, y_sanity)\n",
                "    optim.zero_grad()\n",
                "    loss.backward()\n",
                "    optim.step()\n",
                "    losses.append(loss.item())\n",
                "    \n",
                "    if i % 20 == 0:\n",
                "        print(f\"Iter {i}: Loss {loss.item():.4f} -> '{generate(model, 'To ', 10)}'\")\n",
                "\n",
                "plt.plot(losses)\n",
                "plt.title(\"Curva de Aprendizaje (Sanity Check)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 10. Glass Box: Atlas de Activaci贸n\n",
                "Visualizamos las neuronas del modelo 'Sanity' que acabamos de entrenar."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pasamos el texto una vez m谩s para capturar activaciones\n",
                "model(data)\n",
                "act = model.activations['layer_0'].squeeze(0).numpy()[:, :50].T\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.heatmap(act, cmap='magma')\n",
                "plt.title(\"Actividad Neuronal (Capa 0)\")\n",
                "plt.xlabel(\"Caracteres\")\n",
                "plt.ylabel(\"Neurona\")\n",
                "plt.xticks(ticks=np.arange(len(sanity_text)), labels=list(sanity_text))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11. El Gran Torneo (Configuraci贸n)\n",
                "Definimos los gladiadores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "configs = [\n",
                "    {'name': 'Tiny Mamba', 'n_layers': 2, 'd_model': 64, 'n_heads': 2},\n",
                "    {'name': 'Medium Hybrid', 'n_layers': 4, 'd_model': 128, 'n_heads': 4},\n",
                "    {'name': 'Big Transformer', 'n_layers': 4, 'd_model': 256, 'n_heads': 8}\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 12. Ejecuci贸n del Torneo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "print(\"锔 隆Comienza el Torneo!\")\n",
                "\n",
                "for cfg in configs:\n",
                "    print(f\"\\nEntrenando: {cfg['name']}...\")\n",
                "    m = CortexOrganism(cfg).to(device)\n",
                "    opt = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
                "    \n",
                "    losses = []\n",
                "    for i in range(200): # Sprint corto\n",
                "        xb, yb = get_batch()\n",
                "        _, loss = m(xb, yb)\n",
                "        opt.zero_grad()\n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "        losses.append(loss.item())\n",
                "    \n",
                "    sample = generate(m, \"The \", 30).replace('\\n', ' ')\n",
                "    print(f\"   Final Loss: {losses[-1]:.4f}\")\n",
                "    print(f\"   Muestra: \\\"{sample}...\\\"\")\n",
                "    results.append({'name': cfg['name'], 'loss': losses[-1], 'sample': sample})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 13. Tabla de Resultados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.DataFrame(results).sort_values('loss')\n",
                "display(df)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}