{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ Cortex-11: The Mega-Lab (30-Step Deep Dive)\n",
                "\n",
                "Bienvenido al laboratorio definitivo. Has pedido detalle, rigor y visualizaci√≥n. Aqu√≠ tienes **33 secciones** que desglosan cada √°tomo del proceso.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ†Ô∏è SECCI√ìN 1: FUNDAMENTOS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Librer√≠as y Motor\n",
                "Importamos las herramientas y verificamos si tenemos aceleraci√≥n por hardware (GPU)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import random\n",
                "import requests\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"üöÄ Motor Cortex Iniciado en: {device.upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Reproducibilidad (La Semilla de Dios)\n",
                "Para que este experimento sea ciencia y no magia, fijamos la aleatoriedad."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def set_seed(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
                "    print(f\"üîí Semilla fijada en: {seed}\")\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìö SECCI√ìN 2: DATOS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Descarga del Corpus (Shakespeare)\n",
                "Obtenemos el texto crudo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
                "shakespeare_text = requests.get(url).text\n",
                "print(f\"üìú Shakespeare descargado. Longitud: {len(shakespeare_text)} caracteres.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. An√°lisis del Dataset\n",
                "Antes de entrenar, miramos qu√© vamos a aprender."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Muestra del Texto ---\")\n",
                "print(shakespeare_text[:200])\n",
                "print(\"\\n--- Estad√≠sticas ---\")\n",
                "chars = sorted(list(set(shakespeare_text)))\n",
                "print(f\"Vocabulario ({len(chars)} chars): {''.join(chars)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Tokenizaci√≥n (Byte-Level)\n",
                "Convertimos texto en n√∫meros. Usamos bytes crudos (0-255) para universalidad."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_tensor = torch.tensor([ord(c) for c in shakespeare_text], dtype=torch.long)\n",
                "print(f\"Tensor de Datos: {data_tensor.shape}\")\n",
                "print(f\"Ejemplo: 'First' -> {data_tensor[:5].tolist()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Motor de Batches\n",
                "C√≥mo alimentamos al modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_batch(data, batch_size=32, block_size=64):\n",
                "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
                "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
                "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
                "    return x, y"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß† SECCI√ìN 3: ARQUITECTURAS (EL ARSENAL)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Componente: Mamba Block (SSM)\n",
                "La memoria lineal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MambaBlock(nn.Module):\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_model, d_model * 2)\n",
                "        self.out_proj = nn.Linear(d_model, d_model)\n",
                "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
                "    def forward(self, x):\n",
                "        B, L, D = x.shape\n",
                "        x_and_res = self.in_proj(x)\n",
                "        x_val, res = x_and_res.chunk(2, dim=-1)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = self.conv(x_val)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = F.silu(x_val)\n",
                "        return self.out_proj(x_val * F.sigmoid(res))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Componente: RWKV Block (Linear RNN)\n",
                "La recurrencia eficiente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RWKVBlock(nn.Module):\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.receptance = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.output = nn.Linear(d_model, d_model, bias=False)\n",
                "    def forward(self, x):\n",
                "        k = self.key(x)\n",
                "        v = self.value(x)\n",
                "        r = torch.sigmoid(self.receptance(x))\n",
                "        return self.output(r * (k * v)) # Simplificado"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Componente: MoE Block (Mixture of Experts)\n",
                "Capacidad masiva, coste bajo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Expert(nn.Module):\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.ReLU(), nn.Linear(4*d_model, d_model))\n",
                "    def forward(self, x): return self.net(x)\n",
                "\n",
                "class MoEBlock(nn.Module):\n",
                "    def __init__(self, d_model, num_experts=4):\n",
                "        super().__init__()\n",
                "        self.experts = nn.ModuleList([Expert(d_model) for _ in range(num_experts)])\n",
                "        self.gate = nn.Linear(d_model, num_experts)\n",
                "    def forward(self, x):\n",
                "        weights, indices = torch.topk(self.gate(x), 2, dim=-1)\n",
                "        weights = F.softmax(weights, dim=-1)\n",
                "        out = torch.zeros_like(x)\n",
                "        for i, expert in enumerate(self.experts):\n",
                "            mask = (indices == i).any(dim=-1, keepdim=True)\n",
                "            if mask.any(): out += mask * expert(x) * weights.sum(dim=-1, keepdim=True)\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 10. El Organismo Universal\n",
                "La clase que puede ser cualquier cosa seg√∫n su ADN."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CortexOrganism(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.embedding = nn.Embedding(256, config['d_model'])\n",
                "        self.layers = nn.ModuleList()\n",
                "        for i in range(config['n_layers']): \n",
                "            if config['type'] == 'Transformer':\n",
                "                self.layers.append(nn.TransformerEncoderLayer(config['d_model'], config['n_heads'], dim_feedforward=4*config['d_model'], batch_first=True))\n",
                "            elif config['type'] == 'Mamba': self.layers.append(MambaBlock(config['d_model']))\n",
                "            elif config['type'] == 'RWKV': self.layers.append(RWKVBlock(config['d_model']))\n",
                "            elif config['type'] == 'MoE': self.layers.append(MoEBlock(config['d_model']))\n",
                "            elif config['type'] == 'Hybrid':\n",
                "                if i % 2 == 0: self.layers.append(MambaBlock(config['d_model']))\n",
                "                else: self.layers.append(nn.TransformerEncoderLayer(config['d_model'], config['n_heads'], batch_first=True))\n",
                "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
                "        self.head = nn.Linear(config['d_model'], 256)\n",
                "    def forward(self, idx, targets=None):\n",
                "        x = self.embedding(idx)\n",
                "        for layer in self.layers: x = layer(x)\n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        loss = None\n",
                "        if targets is not None:\n",
                "            B, T, C = logits.shape\n",
                "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
                "        return logits, loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11. Funci√≥n de Generaci√≥n (El Habla)\n",
                "Para ver qu√© dicen los modelos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate(model, prompt, max_len=50):\n",
                "    model.eval()\n",
                "    idx = torch.tensor([ord(c) for c in prompt], dtype=torch.long).unsqueeze(0).to(device)\n",
                "    for _ in range(max_len):\n",
                "        with torch.no_grad():\n",
                "            logits, _ = model(idx)\n",
                "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
                "            next_token = torch.multinomial(probs, 1)\n",
                "            idx = torch.cat((idx, next_token), dim=1)\n",
                "    return \"\".join([chr(i) for i in idx[0].tolist()])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÜ SECCI√ìN 4: EL TORNEO"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 12. Configuraci√≥n de los Clasificatorios\n",
                "Definimos qu√© vamos a buscar."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "architectures = ['Transformer', 'Mamba', 'RWKV', 'MoE', 'Hybrid']\n",
                "print(f\"Gladiadores: {architectures}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 13. Ejecuci√≥n: Clasificatorios (Qualifiers)\n",
                "Corremos 20 versiones de cada uno para encontrar al campe√≥n de cada clase."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "champions = {}\n",
                "for arch in architectures:\n",
                "    print(f\"\\nü•ä Buscando el mejor {arch}...\")\n",
                "    best_loss = float('inf')\n",
                "    best_config = None\n",
                "    \n",
                "    for i in range(5): # 5 Trials por demo (subir a 100)\n",
                "        config = {\n",
                "            'type': arch,\n",
                "            'n_layers': random.choice([2, 4]),\n",
                "            'd_model': 128,\n",
                "            'n_heads': 4,\n",
                "            'lr': 1e-3\n",
                "        }\n",
                "        model = CortexOrganism(config).to(device)\n",
                "        optim = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
                "        \n",
                "        # Sprint de entrenamiento\n",
                "        for _ in range(20):\n",
                "            xb, yb = get_batch(data_tensor)\n",
                "            _, loss = model(xb, yb)\n",
                "            loss.backward(); optim.step(); optim.zero_grad()\n",
                "            \n",
                "        if loss.item() < best_loss:\n",
                "            best_loss = loss.item()\n",
                "            best_config = config\n",
                "            \n",
                "        print(f\"   Trial {i}: Loss {loss.item():.4f}\")\n",
                "        \n",
                "    print(f\"üëë Campe√≥n {arch}: Loss {best_loss:.4f}\")\n",
                "    champions[arch] = best_config"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 14. Preparaci√≥n de la Final (Dataset Matem√°tico)\n",
                "Generamos el reto nuevo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "math_text = \"\".join([f\"Q:{a}+{b}={a+b}\\n\" for a in range(100) for b in range(100)])\n",
                "math_tensor = torch.tensor([ord(c) for c in math_text], dtype=torch.long)\n",
                "print(f\"üßÆ Dataset Matem√°tico: {len(math_text)} chars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 15. LA GRAN FINAL: Ejecuci√≥n\n",
                "Entrenamos a los campeones en Literatura y luego los adaptamos a Matem√°ticas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "print(\"üèüÔ∏è LA GRAN FINAL...\")\n",
                "\n",
                "for arch, config in champions.items():\n",
                "    print(f\"\\nüèÉ Corriendo: {arch}\")\n",
                "    model = CortexOrganism(config).to(device)\n",
                "    optim = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
                "    \n",
                "    # 1. Literatura\n",
                "    w_before = {k: v.clone() for k, v in model.named_parameters()}\n",
                "    for i in range(100):\n",
                "        xb, yb = get_batch(data_tensor)\n",
                "        _, loss = model(xb, yb)\n",
                "        loss.backward(); optim.step(); optim.zero_grad()\n",
                "    loss_lit = loss.item()\n",
                "    sample_lit = generate(model, \"The king \", 30)\n",
                "    \n",
                "    # 2. Matem√°ticas\n",
                "    for i in range(100):\n",
                "        xb, yb = get_batch(math_tensor)\n",
                "        _, loss = model(xb, yb)\n",
                "        loss.backward(); optim.step(); optim.zero_grad()\n",
                "    loss_math = loss.item()\n",
                "    sample_math = generate(model, \"Q:10+10=\", 10)\n",
                "    \n",
                "    # 3. Plasticidad\n",
                "    w_after = {k: v for k, v in model.named_parameters()}\n",
                "    plasticity = sum((w_after[k] - w_before[k]).norm().item() for k in w_before)\n",
                "    \n",
                "    print(f\"   üìú Lit: \\\"{sample_lit}...\\\"\")\n",
                "    print(f\"   üßÆ Math: \\\"{sample_math}...\\\"\")\n",
                "    print(f\"   üß† Plasticity: {plasticity:.2f}\")\n",
                "    \n",
                "    results.append({'Arch': arch, 'Math Loss': loss_math, 'Plasticity': plasticity, 'Sample': sample_math})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 16. An√°lisis de Resultados\n",
                "¬øQui√©n gan√≥?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.DataFrame(results).sort_values('Math Loss')\n",
                "display(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî¨ SECCI√ìN 5: DEEP DIVE (EL GANADOR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 17. Selecci√≥n del Ganador\n",
                "Autom√°ticamente elegimos al mejor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "winner_arch = df.iloc[0]['Arch']\n",
                "winner_config = champions[winner_arch]\n",
                "print(f\"üèÜ El Ganador es: {winner_arch}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 18. Re-entrenamiento del Ganador (Larga Duraci√≥n)\n",
                "Ahora que sabemos qui√©n es el mejor, lo entrenamos en serio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"üèãÔ∏è Entrenando {winner_arch} en serio...\")\n",
                "model = CortexOrganism(winner_config).to(device)\n",
                "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
                "\n",
                "losses = []\n",
                "for i in range(500):\n",
                "    xb, yb = get_batch(data_tensor)\n",
                "    _, loss = model(xb, yb)\n",
                "    loss.backward(); optim.step(); optim.zero_grad()\n",
                "    losses.append(loss.item())\n",
                "    if i % 100 == 0:\n",
                "        print(f\"   Iter {i}: Loss {loss.item():.4f} -> \\\"{generate(model, 'The ', 20)}...\\\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 19. Visualizaci√≥n: Curva de Aprendizaje\n",
                "C√≥mo aprendi√≥ el campe√≥n."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(losses)\n",
                "plt.title(f\"Curva de Aprendizaje ({winner_arch})\")\n",
                "plt.xlabel(\"Iteraciones\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 20. Futuro: Guardado\n",
                "Guardamos el cerebro para la posteridad."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save(model.state_dict(), f\"cortex_winner_{winner_arch}.pth\")\n",
                "print(\"üíæ Modelo guardado.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}