{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcEfWeyJMN0gat+dT60ehm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRamiBalles/llm/blob/main/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Librerías"
      ],
      "metadata": {
        "id": "Ao_GV0ChkoV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jzES_Z8om2n",
        "outputId": "af080200-0e46-48c8-91f6-1ef0147c6bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estamos usando: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c59a3f79770>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Configuramos el dispositivo\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Estamos usando: {device}\")\n",
        "\n",
        "# Para reproducibilidad\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tokens"
      ],
      "metadata": {
        "id": "kusM83w8k4QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de datos\n",
        "with open('datos_sancho_mini.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Longitud del dataset en caracteres: \", len(text))\n",
        "print(\"\\n--- Primeros 700 caracteres del dataset ---\")\n",
        "print(text[:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "-OuOTGsik7Mn",
        "outputId": "dd016ae5-8441-4593-e46a-9f08bd82afd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datos_sancho_mini.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-179778010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Carga de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datos_sancho_mini.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Longitud del dataset en caracteres: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datos_sancho_mini.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6343faeb-58bf-4580-bbad-3fa7d4cec95d",
        "id": "w0QTtyv8mYZF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del dataset en caracteres:  147758\n",
            "\n",
            "--- Primeros 700 caracteres del dataset ---\n",
            "<|inicio|>\n",
            "Era de aspecto venerable y viejo;\n",
            "de verde, azul y plata era el vestido,\n",
            "robusto al parecer y de buen rejo,\n",
            "\n",
            "aunque, como enojado, denegrido\n",
            "se mostraba en el rostro, que la saña\n",
            "así turba el color como el sentido.\n",
            "\n",
            "Airado, contra aquéllos más se ensaña\n",
            "que nadan más, y sáleles al paso,\n",
            "juzgando a gloria tan cobarde hazaña.\n",
            "\n",
            "En esto oh nuevo y milagroso caso,\n",
            "digno de que se cuente poco a poco\n",
            "y con los versos de Torcato Taso\n",
            "\n",
            "Hasta aquí no he invocado, ahora invoco\n",
            "vuestro favor, oh Musas, necesario\n",
            "para los altos puntos en que toco;\n",
            "\n",
            "descerrajad vuestro más rico almario,\n",
            "y el aliento me dad que el caso pide,\n",
            "no humilde, no ratero ni ordinario,\n",
            "<|fin|>\n",
            "\n",
            "<|inicio|>\n",
            "que tiene a sus\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s62XV2sopEHe",
        "outputId": "4362cbfc-340f-4859-8f00-aa7b85e14ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Vocabulario ---\n",
            "\n",
            " ,.;<>ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz|ÁÉÍÑÓÚÜáéíñóúü\n",
            "Tamaño del vocabulario: 70\n",
            "\n",
            "Prueba de tokenización:\n",
            "[5, 55, 39, 43, 39, 33, 39, 44, 55, 6, 0, 11, 47, 31, 1, 34, 35, 1, 31, 48, 45, 35, 33, 49, 44, 1, 51, 35, 43, 35, 47, 31, 32, 41, 35, 1, 53, 1, 51, 39, 35, 40, 44, 4, 0, 34, 35, 1, 51, 35, 47, 34, 35, 2, 1, 31, 54, 50, 41, 1, 53, 1, 45, 41, 31, 49, 31, 1, 35, 47, 31, 1, 35, 41, 1, 51, 35, 48, 49, 39, 34, 44, 2, 0, 47, 44, 32, 50, 48, 49, 44, 1, 31, 41, 1, 45, 31, 47, 35, 33, 35, 47, 1, 53, 1, 34, 35, 1, 32, 50, 35, 43, 1, 47, 35, 40, 44, 2, 0, 0, 31, 50, 43, 46, 50, 35, 2, 1, 33, 44, 42, 44, 1, 35, 43, 44, 40, 31, 34, 44, 2, 1, 34, 35, 43, 35, 37, 47, 39, 34, 44, 0, 48, 35, 1, 42, 44, 48, 49, 47, 31, 32, 31, 1, 35, 43, 1, 35, 41, 1, 47, 44, 48, 49, 47, 44, 2, 1, 46, 50, 35, 1, 41, 31, 1, 48, 31, 66, 31, 0, 31, 48, 65, 1, 49, 50, 47, 32, 31, 1, 35, 41, 1, 33, 44, 41, 44, 47, 1, 33, 44, 42, 44, 1, 35, 41, 1, 48, 35, 43, 49, 39, 34, 44, 3, 0, 0, 7, 39, 47, 31, 34, 44, 2, 1, 33, 44, 43, 49, 47, 31, 1, 31, 46, 50, 64, 41, 41, 44, 48, 1, 42, 63, 48, 1, 48, 35, 1, 35, 43, 48, 31, 66, 31, 0, 46, 50, 35, 1, 43, 31, 34, 31, 43, 1, 42, 63, 48, 2, 1, 53, 1, 48, 63, 41, 35, 41, 35, 48, 1, 31, 41, 1, 45, 31, 48, 44, 2, 0, 40, 50, 54, 37, 31, 43, 34, 44, 1, 31, 1, 37, 41, 44, 47, 39, 31, 1, 49, 31, 43, 1, 33, 44, 32, 31, 47, 34, 35, 1, 38, 31, 54, 31, 66, 31, 3, 0, 0, 11, 43, 1, 35, 48, 49, 44, 1, 44, 38, 1, 43, 50, 35, 51, 44, 1, 53, 1, 42, 39, 41, 31, 37, 47, 44, 48, 44, 1, 33, 31, 48, 44, 2, 0, 34, 39, 37, 43, 44, 1, 34, 35, 1, 46, 50, 35, 1, 48, 35, 1, 33, 50, 35, 43, 49, 35, 1, 45, 44, 33, 44, 1, 31, 1, 45, 44, 33, 44, 0, 53, 1, 33, 44, 43, 1, 41, 44, 48, 1, 51, 35, 47, 48, 44, 48, 1, 34, 35, 1, 25, 44, 47, 33, 31, 49, 44, 1, 25, 31, 48, 44, 0, 0, 14, 31, 48, 49, 31, 1, 31, 46, 50, 65, 1, 43, 44, 1, 38, 35, 1, 39, 43, 51, 44, 33, 31, 34, 44, 2, 1, 31, 38, 44, 47, 31, 1, 39, 43, 51, 44, 33, 44, 0, 51, 50, 35, 48, 49, 47, 44, 1, 36, 31, 51, 44, 47, 2, 1, 44, 38, 1, 18, 50, 48, 31, 48, 2, 1, 43, 35, 33, 35, 48, 31, 47, 39, 44, 0, 45, 31, 47, 31, 1, 41, 44, 48, 1, 31, 41, 49, 44, 48, 1, 45, 50, 43, 49, 44, 48, 1, 35, 43, 1, 46, 50, 35, 1, 49, 44, 33, 44, 4, 0, 0, 34, 35, 48, 33, 35, 47, 47, 31, 40, 31, 34, 1, 51, 50, 35, 48, 49, 47, 44, 1, 42, 63, 48, 1, 47, 39, 33, 44, 1, 31, 41, 42, 31, 47, 39, 44, 2, 0, 53, 1, 35, 41, 1, 31, 41, 39, 35, 43, 49, 44, 1, 42, 35, 1, 34, 31, 34, 1, 46, 50, 35, 1, 35, 41, 1, 33, 31, 48, 44, 1, 45, 39, 34, 35, 2, 0, 43, 44, 1, 38, 50, 42, 39, 41, 34, 35, 2, 1, 43, 44, 1, 47, 31, 49, 35, 47, 44, 1, 43, 39, 1, 44, 47, 34, 39, 43, 31, 47, 39, 44, 2, 0, 5, 55, 36, 39, 43, 55, 6]\n"
          ]
        }
      ],
      "source": [
        "# Obtenemos todos los caracteres únicos que aparecen en el texto\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"--- Vocabulario ---\")\n",
        "print(''.join(chars))\n",
        "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
        "\n",
        "# Creamos el mapeo de caracteres a enteros (tokenización)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # codificador: toma un string, devuelve una lista de enteros\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decodificador: toma una lista de enteros\n",
        "\n",
        "# Probamos nuestra tokenización\n",
        "test_string = \"Hola Sancho\"\n",
        "encoded_string = encode(test_string)\n",
        "decoded_string = decode(encoded_string)\n",
        "print(f\"\\nPrueba de tokenización:\")\n",
        "print(f\"{encoded_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Embeddings"
      ],
      "metadata": {
        "id": "skKSgRPGmncD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ooCY3HhraWN"
      },
      "outputs": [],
      "source": [
        "n_embd = 256\n",
        "block_size = 256\n",
        "\n",
        "# Creamos las tablas de embeddings\n",
        "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "# Ejemplo de cómo se combina:\n",
        "# Tomemos un índice de token de ejemplo y una posición\n",
        "idx_ejemplo = torch.tensor([[encode('Hola')[0]]], dtype=torch.long) # H -> 40\n",
        "pos_ejemplo = torch.arange(0, 1, dtype=torch.long) # Posición 0\n",
        "\n",
        "tok_emb = token_embedding_table(idx_ejemplo) # (1, 1, n_embd)\n",
        "pos_emb = position_embedding_table(pos_ejemplo) # (1, n_embd)\n",
        "\n",
        "x = tok_emb + pos_emb # Así se combinan"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. LayerNorm"
      ],
      "metadata": {
        "id": "Vi7r5gavmufl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awkaIP4KrjQL"
      },
      "outputs": [],
      "source": [
        "# PyTorch ya nos da una implementación eficiente\n",
        "ln_ejemplo = nn.LayerNorm(n_embd)\n",
        "# Aplicamos la normalización a nuestro embedding de ejemplo\n",
        "x_normalizado = ln_ejemplo(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Self Attention"
      ],
      "metadata": {
        "id": "K6a2aY1bmy30"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVO7T5Scm9Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg09N8XgroGH"
      },
      "outputs": [],
      "source": [
        "n_head = 4\n",
        "dropout = 0.2\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Una cabeza de self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C/4)**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Múltiples cabezas de self-attention en paralelo \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. MLP"
      ],
      "metadata": {
        "id": "rknXcLbqm-Id"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ht5GJvWrr_T"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Una red feed-forward simple \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Bloque Transformer"
      ],
      "metadata": {
        "id": "MHVFets0nFQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Un bloque de Transformer: comunicación seguida de computación \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Primera parte: Self-Attention\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # Segunda parte: Feed-Forward\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "SabgzvCanEkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anJS87v3nR0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgIan78kr0sU"
      },
      "outputs": [],
      "source": [
        "n_layer = 4\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            # El softmax se aplica aquí para convertir logits en probabilidades\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Creamos la instancia del modelo y la movemos al dispositivo\n",
        "model = GPTLanguageModel()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8. Trainning"
      ],
      "metadata": {
        "id": "_PBrD4CRnWIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKB5PjesG9n",
        "outputId": "f70b49b2-3113-4948-98ea-4b56fa60227a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ejemplo de un lote de datos ---\n",
            "Entradas (shape): torch.Size([32, 256])\n",
            "Targets (shape): torch.Size([32, 256])\n"
          ]
        }
      ],
      "source": [
        "# Primero, convertimos todo nuestro texto tokenizado en tensores de PyTorch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Probamos nuestra función\n",
        "xb, yb = get_batch('train')\n",
        "print(\"--- Ejemplo de un lote de datos ---\")\n",
        "print('Entradas (shape):', xb.shape)\n",
        "print('Targets (shape):', yb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd-tMcucsM3M",
        "outputId": "06b2b220-c12a-4b2b-a1d7-fbf7492cc455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo creado con 3.26M de parámetros.\n",
            "Paso 0: pérdida de entrenamiento 4.3902, pérdida de validación 4.3945\n",
            "Paso 200: pérdida de entrenamiento 2.3039, pérdida de validación 2.3295\n",
            "Paso 400: pérdida de entrenamiento 2.1847, pérdida de validación 2.2147\n",
            "Paso 600: pérdida de entrenamiento 2.0520, pérdida de validación 2.0863\n",
            "Paso 800: pérdida de entrenamiento 1.9414, pérdida de validación 1.9863\n",
            "Paso 1000: pérdida de entrenamiento 1.8539, pérdida de validación 1.9126\n",
            "Paso 1200: pérdida de entrenamiento 1.7761, pérdida de validación 1.8423\n",
            "Paso 1400: pérdida de entrenamiento 1.7127, pérdida de validación 1.8018\n",
            "Paso 1600: pérdida de entrenamiento 1.6375, pérdida de validación 1.7410\n",
            "Paso 1800: pérdida de entrenamiento 1.5734, pérdida de validación 1.7012\n",
            "Paso 2000: pérdida de entrenamiento 1.5168, pérdida de validación 1.6712\n",
            "Paso 2200: pérdida de entrenamiento 1.4650, pérdida de validación 1.6374\n",
            "Paso 2400: pérdida de entrenamiento 1.4166, pérdida de validación 1.6229\n",
            "Paso 2600: pérdida de entrenamiento 1.3644, pérdida de validación 1.6000\n",
            "Paso 2800: pérdida de entrenamiento 1.3219, pérdida de validación 1.5925\n",
            "Paso 3000: pérdida de entrenamiento 1.2791, pérdida de validación 1.5817\n",
            "Paso 3200: pérdida de entrenamiento 1.2396, pérdida de validación 1.5776\n",
            "Paso 3400: pérdida de entrenamiento 1.1979, pérdida de validación 1.5735\n",
            "Paso 3600: pérdida de entrenamiento 1.1650, pérdida de validación 1.5900\n",
            "Paso 3800: pérdida de entrenamiento 1.1240, pérdida de validación 1.5853\n",
            "Paso 4000: pérdida de entrenamiento 1.0872, pérdida de validación 1.5931\n",
            "Paso 4200: pérdida de entrenamiento 1.0503, pérdida de validación 1.6089\n",
            "Paso 4400: pérdida de entrenamiento 1.0124, pérdida de validación 1.6182\n",
            "Paso 4600: pérdida de entrenamiento 0.9770, pérdida de validación 1.6303\n",
            "Paso 4800: pérdida de entrenamiento 0.9451, pérdida de validación 1.6511\n",
            "Paso 4999: pérdida de entrenamiento 0.9063, pérdida de validación 1.6574\n",
            "\n",
            "--- ¡Entrenamiento completado! ---\n"
          ]
        }
      ],
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "\n",
        "m = model.to(device)\n",
        "print(f\"Modelo creado con {sum(p.numel() for p in m.parameters())/1e6:.2f}M de parámetros.\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Paso {iter}: pérdida de entrenamiento {losses['train']:.4f}, pérdida de validación {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"\\n--- ¡Entrenamiento completado! ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Generación\n",
        "\n"
      ],
      "metadata": {
        "id": "ewopeSxunNtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7_LS944sQBe",
        "outputId": "28848bd9-6dff-4062-dced-d8e05cf02029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sancho-mini generando texto :D\n",
            "\n",
            "\n",
            "<|inicio|>\n",
            "de la espada en ella alta y la tierra\n",
            "de la industria y entente brava\n",
            "la fama que el aire de la cabeza lleva.\n",
            "\n",
            "Mas no sé que estáis que es de aquí encierra\n",
            "de la más nombre está ninguno\n",
            "de aquellos tan tiene con la escucha,\n",
            "\n",
            "de su guía el discreto se aprecia\n",
            "el gran caso de la ocasión te ha vistoria.\n",
            "\n",
            "En esto, que de los casos de los guía\n",
            "el bajel que el cielo es de Apolo estaba,\n",
            "y así le tierra en file corriente.\n",
            "\n",
            "Cuatro vi tan al vez de la cabeza,\n",
            "que sobre el cielo el cielo, y el cielo,\n",
            "y en esto de la caballeza y crece,\n",
            "\n",
            "por ver la vencidad y de la atrevida,\n",
            "para en la canalla y la vitoria\n",
            "de la caballeza y la fama por vida,\n",
            "<|fin|>\n",
            "\n",
            "<|inicio|>\n",
            "de la caballera y la fama fuente,\n",
            "y la muerte al cielo a la vitoria.\n",
            "\n",
            "Mas no se le tierra en este repiese\n",
            "a la canalla y con la vitoria y la fama\n",
            "de la cabeza que el alma compaña;\n",
            "\n",
            "de la caballera más de la cabeza\n",
            "la vista, que el mundo el corte aprieto\n",
            "de la insigne de la fama en poeta,\n",
            "<|fin|>\n",
            "\n",
            "<|inicio|>\n",
            "de esta verdad alguna y "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "print(\"\\nsancho-mini generando texto :D\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Cuanto más baja la temperatura más coherente, pero más predecible\n",
        "temperature = 0.4\n",
        "top_k = 50\n",
        "\n",
        "start_token = encode('<|startofpoem|>')[0]\n",
        "context = torch.tensor([[start_token]], dtype=torch.long, device=device)\n",
        "\n",
        "initial_char = decode(context[0].tolist())\n",
        "print(initial_char, end='')\n",
        "sys.stdout.flush()\n",
        "\n",
        "m.eval()\n",
        "with torch.no_grad():\n",
        "    for _ in range(1000):\n",
        "        idx_cond = context[:, -block_size:]\n",
        "        logits, _ = m(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        logits = logits / temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf') # Anula los logits que no están en el top-k\n",
        "\n",
        "        # Aplicamos softmax a los logits ya modificados para obtener probabilidades\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        char = decode(idx_next[0].tolist())\n",
        "\n",
        "        print(char, end='')\n",
        "        sys.stdout.flush()\n",
        "        context = torch.cat((context, idx_next), dim=1)\n",
        "        time.sleep(0.02)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Guardado final."
      ],
      "metadata": {
        "id": "j1YaBE8an2w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub -q\n",
        "\n",
        "# Iniciar sesión en tu cuenta de Hugging Face\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "XnOBrNA2nwhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, create_repo, upload_file\n",
        "import torch\n",
        "\n",
        "best_model_path = \"sancho-mini-best.pth\" # O el nombre que le hayas dado\n",
        "\n",
        "MODEL_NAME = \"sancho-mini.pth\"\n",
        "torch.save(m.state_dict(), MODEL_NAME)\n",
        "print(f\"Modelo guardado en '{MODEL_NAME}'\")\n",
        "\n",
        "# Creamos la tarjets de contenido para el modelo (README.md)\n",
        "\n",
        "model_card_content = f\"\"\"\n",
        "---\n",
        "license: mit\n",
        "language: es\n",
        "pipeline_tag: text-generation\n",
        "---\n",
        "\n",
        "# sancho-mini: Un GPT a nivel de carácter que escribe sonetos como Cervantes\n",
        "\n",
        "Este es un modelo Generative Pre-trained Transformer (GPT) entrenado desde cero con PyTorch para generar sonetos en español.\n",
        "\n",
        "## Detalles del Modelo\n",
        "\n",
        "- **Arquitectura**: GPT (Decoder-only Transformer)\n",
        "- **Nivel de Tokenización**: Carácter\n",
        "- **Tamaño del Vocabulario**: {vocab_size}\n",
        "- **Dimensión de Embedding (`n_embd`)**: {n_embd}\n",
        "- **Longitud de Contexto (`block_size`)**: {block_size}\n",
        "- **Número de Capas Transformer (`n_layer`)**: {n_layer}\n",
        "- **Número de Cabezas de Atención (`n_head`)**: {n_head}\n",
        "- **Tasa de Dropout**: {dropout}\n",
        "- **Número de Parámetros**: {sum(p.numel() for p in m.parameters())/1e6:.2f}M\n",
        "\n",
        "## Datos de Entrenamiento\n",
        "\n",
        "El modelo fue entrenado con un corpus de sonetos en español (`datos_sancho_mini.txt`), los poemas en el dataset están estructurados con tokens especiales para indicar el inicio y el fin de cada poema.\n",
        "\n",
        "## Uso\n",
        "\n",
        "Para usar este modelo, necesitas cargar el `state_dict` en una instancia de la arquitectura del modelo definida en el notebook de entrenamiento.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "try:\n",
        "    with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(model_card_content)\n",
        "    print(\"Archivo 'README.md' creado correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al escribir README.md: {e}\")\n",
        "\n",
        "\n",
        "# Necesitamos el nombre de usuario\n",
        "\n",
        "hf_username = input(\"Por favor introduce tu nombre de usuario de Hugging Face: \")\n",
        "repo_name = \"sancho-mini-gpt\"\n",
        "repo_id = f\"{hf_username}/{repo_name}\"\n",
        "\n",
        "try:\n",
        "    # Crear el repositorio\n",
        "    repo_url = create_repo(repo_id, private=False, exist_ok=True) # Lo pongo público para que se vea fácil\n",
        "    print(f\"Repositorio creado (o ya existente): {repo_url}\")\n",
        "\n",
        "    # Subir el archivo del modelo\n",
        "    upload_file(\n",
        "        path_or_fileobj=MODEL_NAME,\n",
        "        path_in_repo=MODEL_NAME,\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "    print(f\"Archivo '{MODEL_NAME}' subido al repositorio.\")\n",
        "\n",
        "    # Subir la tarjeta de modelo\n",
        "    upload_file(\n",
        "        path_or_fileobj=\"README.md\",\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTu modelo está ahora en Hugging Face, puedes verlo aquí: {repo_url}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Problema: {e}\")\n",
        "    print(\"Asegúrate de haber introducido correctamente tu nombre de usuario y de tener un token con permisos de escritura.\")"
      ],
      "metadata": {
        "id": "l0IIRTXmn8uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para generación si el modelo ya está entrenado :"
      ],
      "metadata": {
        "id": "1ozXFGUVoDF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "print(f\"Modelo creado con {sum(p.numel() for p in model.parameters())/1e6:.2f}M de parámetros.\")\n",
        "\n",
        "MODEL_PATH = \"sancho-mini.pth\"\n",
        "\n",
        "#    map_location=device asegura que el modelo se cargue en la GPU si está disponible.\n",
        "print(f\"Cargando pesos del modelo desde '{MODEL_PATH}'...\")\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "print(\"¡Pesos cargados correctamente!\")\n",
        "\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aclqSBbt_dmV",
        "outputId": "9c626cc1-3d2d-4e7c-d50f-2983c68e9634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo creado con 3.26M de parámetros.\n",
            "Cargando pesos del modelo desde 'sancho-mini.pth'...\n",
            "¡Pesos cargados correctamente!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "print(\"\\nsancho-mini generando texto :D\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Parámetros de generación\n",
        "temperature = 0.4\n",
        "top_k = 50\n",
        "\n",
        "# Ponemos el modelo en modo de evaluación\n",
        "# Esto es importante porque desactiva capas como Dropout\n",
        "m.eval()\n",
        "\n",
        "# Token de inicio y contexto inicial\n",
        "start_token = encode('<|startofpoem|>')[0]\n",
        "context = torch.tensor([[start_token]], dtype=torch.long, device=device)\n",
        "\n",
        "initial_char = decode(context[0].tolist())\n",
        "print(initial_char, end='')\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Generamos texto en un bloque no_grad para mayor eficiencia\n",
        "with torch.no_grad():\n",
        "    for _ in range(1000):\n",
        "        idx_cond = context[:, -block_size:]\n",
        "        logits, _ = m(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        logits = logits / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        char = decode(idx_next[0].tolist())\n",
        "\n",
        "        print(char, end='')\n",
        "        sys.stdout.flush()\n",
        "        context = torch.cat((context, idx_next), dim=1)\n",
        "        time.sleep(0.02)\n"
      ],
      "metadata": {
        "id": "oxMuDUY1oMrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}