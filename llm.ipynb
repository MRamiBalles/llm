{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß¨ Cortex-2: Advanced Evolutionary Research Lab\n",
                "\n",
                "## üìú Estado del Arte y Justificaci√≥n Cient√≠fica\n",
                "Este entorno implementa una arquitectura de frontera basada en los siguientes papers disruptivos:\n",
                "\n",
                "1.  **Mamba (SSM)**: *Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\".* [arXiv:2312.00752](https://arxiv.org/abs/2312.00752)\n",
                "    *   *Por qu√©*: Resuelve el cuello de botella cuadr√°tico de los Transformers ($O(N^2)$) permitiendo contextos infinitos con coste lineal ($O(N)$).\n",
                "2.  **Mixture of Experts (MoE)**: *Shazeer et al. (2017). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\".* [arXiv:1701.06538](https://arxiv.org/abs/1701.06538)\n",
                "    *   *Por qu√©*: Desacopla la capacidad de computaci√≥n (FLOPs) de la capacidad de memoria (Par√°metros). Permite modelos gigantes que corren r√°pido.\n",
                "3.  **Byte-Level Modeling**: *Xue et al. (2022). \"ByT5: Towards a Token-Free Future\".* [arXiv:2105.13626](https://arxiv.org/abs/2105.13626)\n",
                "    *   *Por qu√©*: Elimina el sesgo humano del Tokenizer. Hace al modelo robusto a \"ruido\" y multiling√ºe por defecto.\n",
                "4.  **Neural Architecture Search (NAS)**: *Real et al. (2019). \"Regularized Evolution for Image Classifier Architecture Search\".* [arXiv:1802.01548](https://arxiv.org/abs/1802.01548)\n",
                "    *   *Por qu√©*: La intuici√≥n humana falla en espacios de alta dimensi√≥n. La evoluci√≥n encuentra √≥ptimos locales que nosotros ignoramos.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 0. Configuraci√≥n e Importaciones\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import random\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.graph_objects as go\n",
                "import plotly.express as px\n",
                "import pandas as pd\n",
                "from sklearn.decomposition import PCA\n",
                "from IPython.display import clear_output, display\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"üöÄ Cortex-2 Engine Active on: {device.upper()}\")\n",
                "\n",
                "def set_seed(seed=42):\n",
                "    \"\"\" Garantiza reproducibilidad total \"\"\"\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "    print(f\"üîí Semilla fijada en: {seed}\")\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß† Arquitectura Modular (Caja Blanca)\n",
                "Hemos instrumentado el c√≥digo para extraer **telemetr√≠a interna**. No es solo \"forward pass\", es un esc√°ner cerebral."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Componentes Instrumentados ---\n",
                "\n",
                "class InstrumentedAttention(nn.Module):\n",
                "    \"\"\" Atenci√≥n con captura de mapas de calor \"\"\"\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.mha = nn.MultiheadAttention(config['d_model'], config['n_heads'], batch_first=True)\n",
                "        self.last_attn_weights = None\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Capturamos los pesos de atenci√≥n (Average across heads for simplicity in visualization)\n",
                "        out, weights = self.mha(x, x, x, need_weights=True, average_attn_weights=True)\n",
                "        self.last_attn_weights = weights.detach().cpu()\n",
                "        return out\n",
                "\n",
                "class MambaBlock(nn.Module):\n",
                "    \"\"\" Bloque Mamba Simplificado \"\"\"\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_model, d_model * 2)\n",
                "        self.out_proj = nn.Linear(d_model, d_model)\n",
                "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        B, L, D = x.shape\n",
                "        x_and_res = self.in_proj(x)\n",
                "        x_val, res = x_and_res.chunk(2, dim=-1)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = self.conv(x_val)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = F.silu(x_val)\n",
                "        return self.out_proj(x_val * F.sigmoid(res))\n",
                "\n",
                "class MoELayer(nn.Module):\n",
                "    \"\"\" MoE con Telemetr√≠a de Routing \"\"\"\n",
                "    def __init__(self, d_model, n_experts, top_k=2):\n",
                "        super().__init__()\n",
                "        self.experts = nn.ModuleList([\n",
                "            nn.Sequential(\n",
                "                nn.Linear(d_model, 4 * d_model), nn.GELU(), \n",
                "                nn.Linear(4 * d_model, d_model), nn.Dropout(0.1)\n",
                "            ) for _ in range(n_experts)\n",
                "        ])\n",
                "        self.gate = nn.Linear(d_model, n_experts)\n",
                "        self.top_k = top_k\n",
                "        self.last_routing_dist = None\n",
                "\n",
                "    def forward(self, x):\n",
                "        gate_logits = self.gate(x)\n",
                "        weights, indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
                "        weights = F.softmax(weights, dim=-1)\n",
                "        \n",
                "        # Telemetr√≠a: ¬øQu√© expertos se activaron?\n",
                "        self.last_routing_dist = indices.detach().cpu().view(-1).bincount(minlength=len(self.experts))\n",
                "        \n",
                "        out = torch.zeros_like(x)\n",
                "        for i, expert in enumerate(self.experts):\n",
                "            mask = (indices == i).any(dim=-1)\n",
                "            if mask.any():\n",
                "                out[mask] += expert(x[mask])\n",
                "        return out\n",
                "\n",
                "class CortexOrganism(nn.Module):\n",
                "    def __init__(self, dna):\n",
                "        super().__init__()\n",
                "        self.dna = dna\n",
                "        self.embedding = nn.Embedding(256, dna['d_model'])\n",
                "        self.layers = nn.ModuleList()\n",
                "        \n",
                "        for i in range(dna['n_layers']):\n",
                "            if dna['backbone'] == 'mamba':\n",
                "                self.layers.append(MambaBlock(dna['d_model']))\n",
                "            elif dna['backbone'] == 'hybrid' and i % 2 == 0:\n",
                "                self.layers.append(MambaBlock(dna['d_model']))\n",
                "            else:\n",
                "                # Usamos nuestra Atenci√≥n Instrumentada\n",
                "                self.layers.append(InstrumentedAttention(dna))\n",
                "        \n",
                "        if dna['moe_experts'] > 0:\n",
                "            self.final_layer = MoELayer(dna['d_model'], dna['moe_experts'])\n",
                "        else:\n",
                "            self.final_layer = nn.Linear(dna['d_model'], dna['d_model'])\n",
                "            \n",
                "        self.ln_f = nn.LayerNorm(dna['d_model'])\n",
                "        self.head = nn.Linear(dna['d_model'], 256)\n",
                "\n",
                "    def forward(self, idx, targets=None):\n",
                "        x = self.embedding(idx)\n",
                "        for layer in self.layers:\n",
                "            x = layer(x)\n",
                "        if self.dna['moe_experts'] > 0:\n",
                "            x = self.final_layer(x)\n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        \n",
                "        loss = None\n",
                "        if targets is not None:\n",
                "            B, T, C = logits.shape\n",
                "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
                "        return logits, loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî¨ Visualizaci√≥n Avanzada\n",
                "Aqu√≠ definimos las herramientas para inspeccionar el modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_attention(model, input_bytes):\n",
                "    \"\"\" Muestra qu√© bytes miran a qu√© bytes \"\"\"\n",
                "    # Buscar la √∫ltima capa de atenci√≥n\n",
                "    attn_layer = None\n",
                "    for layer in model.layers:\n",
                "        if isinstance(layer, InstrumentedAttention):\n",
                "            attn_layer = layer\n",
                "            \n",
                "    if attn_layer is None or attn_layer.last_attn_weights is None:\n",
                "        print(\"‚ö†Ô∏è No hay capas de atenci√≥n activas o registradas.\")\n",
                "        return\n",
                "\n",
                "    weights = attn_layer.last_attn_weights[0] # Primer batch\n",
                "    \n",
                "    plt.figure(figsize=(10, 8))\n",
                "    sns.heatmap(weights.numpy(), cmap='viridis')\n",
                "    plt.title(\"Mapa de Calor de Atenci√≥n (Razonamiento)\")\n",
                "    plt.xlabel(\"Key Token\")\n",
                "    plt.ylabel(\"Query Token\")\n",
                "    plt.show()\n",
                "\n",
                "def visualize_moe_routing(model):\n",
                "    \"\"\" Muestra la carga de trabajo de cada experto \"\"\"\n",
                "    if not hasattr(model, 'final_layer') or not isinstance(model.final_layer, MoELayer):\n",
                "        return\n",
                "        \n",
                "    dist = model.final_layer.last_routing_dist\n",
                "    if dist is None: return\n",
                "    \n",
                "    plt.figure(figsize=(8, 4))\n",
                "    plt.bar(range(len(dist)), dist.numpy(), color='#818cf8')\n",
                "    plt.title(\"Distribuci√≥n de Carga de Expertos (MoE)\")\n",
                "    plt.xlabel(\"ID del Experto\")\n",
                "    plt.ylabel(\"Tokens Procesados\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öîÔ∏è M√≥dulo 4: El Torneo Evolutivo\n",
                "Aqu√≠ ocurre la magia de la selecci√≥n natural."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Genoma y Bucle Evolutivo ---\n",
                "class Genome:\n",
                "    def __init__(self):\n",
                "        self.genes = {\n",
                "            'n_layers': [2, 4, 6], \n",
                "            'd_model': [128, 256], \n",
                "            'n_heads': [2, 4], \n",
                "            'backbone': ['transformer', 'hybrid'], \n",
                "            'moe_experts': [0, 4, 8], \n",
                "            'learning_rate': [1e-3, 3e-4]\n",
                "        }\n",
                "        self.dna = {k: random.choice(v) for k, v in self.genes.items()}\n",
                "        self.fitness = 0.0\n",
                "    \n",
                "    def mutate(self):\n",
                "        k = random.choice(list(self.genes.keys()))\n",
                "        self.dna[k] = random.choice(self.genes[k])\n",
                "        return f\"üß¨ Mutaci√≥n: {k} -> {self.dna[k]}\"\n",
                "        \n",
                "    def crossover(self, other):\n",
                "        child = Genome()\n",
                "        for k in self.genes:\n",
                "            child.dna[k] = self.dna[k] if random.random() > 0.5 else other.dna[k]\n",
                "        return child\n",
                "\n",
                "# Datos Dummy\n",
                "dummy_data = torch.randint(0, 256, (1000,), dtype=torch.long)\n",
                "def get_batch():\n",
                "    ix = torch.randint(len(dummy_data) - 32, (16,))\n",
                "    x = torch.stack([dummy_data[i:i+32] for i in ix]).to(device)\n",
                "    y = torch.stack([dummy_data[i+1:i+33] for i in ix]).to(device)\n",
                "    return x, y\n",
                "\n",
                "def run_evolution(generations=3, pop_size=4):\n",
                "    population = [Genome() for _ in range(pop_size)]\n",
                "    \n",
                "    for gen in range(generations):\n",
                "        print(f\"\\nüèÅ Generaci√≥n {gen+1}\")\n",
                "        for i, genome in enumerate(population):\n",
                "            model = CortexOrganism(genome.dna).to(device)\n",
                "            optim = torch.optim.AdamW(model.parameters(), lr=genome.dna['learning_rate'])\n",
                "            \n",
                "            # Sprint Training\n",
                "            losses = []\n",
                "            for _ in range(10):\n",
                "                xb, yb = get_batch()\n",
                "                _, loss = model(xb, yb)\n",
                "                optim.zero_grad()\n",
                "                loss.backward()\n",
                "                optim.step()\n",
                "                losses.append(loss.item())\n",
                "            \n",
                "            genome.fitness = sum(losses[-3:]) / 3\n",
                "            print(f\"   Individuo {i} ({genome.dna['backbone']}): Loss {genome.fitness:.4f}\")\n",
                "        \n",
                "        # Selecci√≥n\n",
                "        population.sort(key=lambda x: x.fitness)\n",
                "        survivors = population[:pop_size//2]\n",
                "        \n",
                "        # Reproducci√≥n\n",
                "        new_pop = survivors[:]\n",
                "        while len(new_pop) < pop_size:\n",
                "            parent = random.choice(survivors)\n",
                "            child = parent.crossover(random.choice(survivors))\n",
                "            if random.random() < 0.4: child.mutate()\n",
                "            new_pop.append(child)\n",
                "        population = new_pop\n",
                "\n",
                "# run_evolution() # Descomentar para correr el torneo de nuevo"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÜ Fase 5: Entrenamiento Final (Producci√≥n)\n",
                "\n",
                "Una vez que la evoluci√≥n ha encontrado el \"ADN Perfecto\", lo usamos para entrenar el modelo final de forma seria y reproducible."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Pegar aqu√≠ el ADN Ganador (Resultado de la Evoluci√≥n)\n",
                "winning_dna = {'n_layers': 2, 'd_model': 256, 'n_heads': 8, 'backbone': 'hybrid', 'moe_experts': 0, 'learning_rate': 0.001}\n",
                "\n",
                "# 2. Fijar Semilla para Reproducibilidad\n",
                "set_seed(42)\n",
                "\n",
                "# 3. Instanciar Modelo Final\n",
                "print(f\"üèóÔ∏è Construyendo Cortex-Final con ADN: {winning_dna}\")\n",
                "final_model = CortexOrganism(winning_dna).to(device)\n",
                "optimizer = torch.optim.AdamW(final_model.parameters(), lr=winning_dna['learning_rate'])\n",
                "\n",
                "# 4. Entrenamiento Largo (1000 Iteraciones)\n",
                "print(\"üî• Iniciando Entrenamiento de Producci√≥n...\")\n",
                "losses = []\n",
                "for i in range(1000):\n",
                "    xb, yb = get_batch()\n",
                "    _, loss = final_model(xb, yb)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    losses.append(loss.item())\n",
                "    \n",
                "    if i % 100 == 0:\n",
                "        print(f\"Iter {i}: Loss {loss.item():.4f}\")\n",
                "\n",
                "# 5. Guardar Modelo\n",
                "torch.save(final_model.state_dict(), \"cortex_final.pth\")\n",
                "print(\"üíæ Modelo guardado como 'cortex_final.pth'\")\n",
                "\n",
                "# 6. Visualizar Curva Final\n",
                "plt.plot(losses)\n",
                "plt.title(\"Curva de Aprendizaje Final\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Inferencia (Prueba de Fuego)\n",
                "def generate(model, prompt, max_len=100):\n",
                "    model.eval()\n",
                "    idx = torch.tensor([b for b in prompt.encode('utf-8')], dtype=torch.long).unsqueeze(0).to(device)\n",
                "    for _ in range(max_len):\n",
                "        with torch.no_grad():\n",
                "            logits, _ = model(idx)\n",
                "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
                "            next_token = torch.multinomial(probs, 1)\n",
                "            idx = torch.cat((idx, next_token), dim=1)\n",
                "    return bytes(idx[0].tolist()).decode('utf-8', errors='replace')\n",
                "\n",
                "print(generate(final_model, \"Artificial Intelligence is\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}