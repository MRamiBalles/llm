{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Cortex-5: Deep Glass Box & Diagnostics\n",
                "\n",
                "## üî¨ El Microscopio Profundo\n",
                "El usuario ha planteado una duda cr√≠tica: *\"¬øPor qu√© el modelo no genera nada coherente a√∫n?\"*.\n",
                "Para responder, hemos implementado una suite de diagn√≥sticos profundos:\n",
                "\n",
                "1.  **Sanity Check (Prueba de Cordura)**: Antes de intentar aprender Shakespeare, el modelo debe demostrar que puede memorizar una sola frase (\"To be or not to be\"). Si no puede hacer esto (Loss -> 0), la arquitectura est√° rota.\n",
                "2.  **Atlas de Activaci√≥n**: Visualizaremos paso a paso qu√© neuronas se encienden para cada letra. Veremos el \"pensamiento\" en tiempo real.\n",
                "3.  **Velocidad del Flujo Residual**: Mediremos cu√°nto contribuye cada capa al resultado final. ¬øEst√°n trabajando todas las capas o algunas son \"pasajeros\"?\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 0. Configuraci√≥n\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random\n",
                "import requests\n",
                "from IPython.display import clear_output, display\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"üöÄ Cortex-5 Engine: {device.upper()}\")\n",
                "\n",
                "def set_seed(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Arquitectura Instrumentada (Glass Box)\n",
                "# Modificamos el modelo para que guarde sus estados internos\n",
                "\n",
                "class MambaBlock(nn.Module):\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_model, d_model * 2)\n",
                "        self.out_proj = nn.Linear(d_model, d_model)\n",
                "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
                "    def forward(self, x):\n",
                "        B, L, D = x.shape\n",
                "        x_and_res = self.in_proj(x)\n",
                "        x_val, res = x_and_res.chunk(2, dim=-1)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = self.conv(x_val)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = F.silu(x_val)\n",
                "        return self.out_proj(x_val * F.sigmoid(res))\n",
                "\n",
                "class CortexOrganism(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.embedding = nn.Embedding(256, config['d_model'])\n",
                "        self.layers = nn.ModuleList()\n",
                "        \n",
                "        # Telemetr√≠a\n",
                "        self.activations = {} # Guardar√° la salida de cada capa\n",
                "        self.residual_velocities = [] # Guardar√° cu√°nto cambia la se√±al en cada capa\n",
                "\n",
                "        for i in range(config['n_layers']): \n",
                "            if i % 2 == 0: self.layers.append(MambaBlock(config['d_model']))\n",
                "            else: self.layers.append(nn.TransformerEncoderLayer(\n",
                "                d_model=config['d_model'], nhead=config['n_heads'], \n",
                "                dim_feedforward=4*config['d_model'], batch_first=True, dropout=0.0\n",
                "            ))\n",
                "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
                "        self.head = nn.Linear(config['d_model'], 256)\n",
                "\n",
                "    def forward(self, idx, targets=None):\n",
                "        # Reset telemetr√≠a\n",
                "        self.activations = {}\n",
                "        self.residual_velocities = []\n",
                "        \n",
                "        x = self.embedding(idx)\n",
                "        \n",
                "        for i, layer in enumerate(self.layers):\n",
                "            prev_x = x\n",
                "            x = layer(x)\n",
                "            \n",
                "            # Capturar telemetr√≠a\n",
                "            with torch.no_grad():\n",
                "                # 1. Activaciones (Neuronas)\n",
                "                self.activations[f\"layer_{i}\"] = x.detach().cpu()\n",
                "                # 2. Velocidad Residual (Norma de la actualizaci√≥n)\n",
                "                # En Transformer/Mamba est√°ndar es x + layer(x), aqu√≠ simplificado para demo\n",
                "                # Asumimos que la capa devuelve la actualizaci√≥n o el nuevo estado\n",
                "                diff = (x - prev_x).norm(dim=-1).mean().item()\n",
                "                self.residual_velocities.append(diff)\n",
                "\n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        loss = None\n",
                "        if targets is not None:\n",
                "            B, T, C = logits.shape\n",
                "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
                "        return logits, loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Fase 1: Sanity Check (Overfitting)\n",
                "Vamos a forzar al modelo a memorizar una sola frase. Si el Loss no baja a 0, no tiene sentido seguir."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sanity_check():\n",
                "    print(\"üè• Iniciando Sanity Check...\")\n",
                "    text = \"To be or not to be, that is the question.\"\n",
                "    data = torch.tensor([ord(c) for c in text], dtype=torch.long).unsqueeze(0).to(device)\n",
                "    x = data[:, :-1]\n",
                "    y = data[:, 1:]\n",
                "    \n",
                "    # Modelo peque√±o para prueba r√°pida\n",
                "    config = {'n_layers': 2, 'd_model': 128, 'n_heads': 4, 'backbone': 'hybrid'}\n",
                "    model = CortexOrganism(config).to(device)\n",
                "    optim = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
                "    \n",
                "    losses = []\n",
                "    for i in range(100):\n",
                "        _, loss = model(x, y)\n",
                "        optim.zero_grad()\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "        losses.append(loss.item())\n",
                "        \n",
                "        if i % 10 == 0:\n",
                "            print(f\"   Iter {i}: Loss {loss.item():.6f}\")\n",
                "            \n",
                "    plt.plot(losses)\n",
                "    plt.title(\"Sanity Check Loss (Debe llegar a 0)\")\n",
                "    plt.show()\n",
                "    \n",
                "    if losses[-1] < 0.01:\n",
                "        print(\"‚úÖ PRUEBA SUPERADA: El modelo es capaz de aprender.\")\n",
                "        return model, text\n",
                "    else:\n",
                "        print(\"‚ùå FALLO CR√çTICO: El modelo no puede memorizar una frase simple.\")\n",
                "        return None, None\n",
                "\n",
                "sanity_model, sanity_text = sanity_check()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî¨ Fase 2: Visualizaci√≥n de Din√°mica Neuronal\n",
                "Ahora que sabemos que funciona, veamos **c√≥mo** funciona por dentro usando el modelo del Sanity Check."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_neuron_dynamics(model, text):\n",
                "    if model is None: return\n",
                "    \n",
                "    # Correr una inferencia para capturar activaciones\n",
                "    data = torch.tensor([ord(c) for c in text], dtype=torch.long).unsqueeze(0).to(device)\n",
                "    model(data) # Forward pass llena self.activations\n",
                "    \n",
                "    # 1. Atlas de Activaci√≥n (Capa 0 - Mamba)\n",
                "    # Mostramos las primeras 50 neuronas para cada caracter\n",
                "    act = model.activations['layer_0'].squeeze(0).numpy()[:, :50].T\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.heatmap(act, cmap='magma', cbar=True)\n",
                "    plt.title(\"Atlas de Activaci√≥n Neuronal (Capa 0: Mamba)\")\n",
                "    plt.xlabel(\"Secuencia de Texto (Caracteres)\")\n",
                "    plt.ylabel(\"ID de Neurona (0-50)\")\n",
                "    # Poner los caracteres en el eje X\n",
                "    plt.xticks(ticks=np.arange(len(text))+0.5, labels=list(text), rotation=0)\n",
                "    plt.show()\n",
                "    \n",
                "    # 2. Velocidad del Flujo Residual\n",
                "    plt.figure(figsize=(8, 4))\n",
                "    plt.bar(range(len(model.residual_velocities)), model.residual_velocities, color='#4ade80')\n",
                "    plt.title(\"Contribuci√≥n por Capa (Residual Velocity)\")\n",
                "    plt.xlabel(\"Capa\")\n",
                "    plt.ylabel(\"Magnitud del Cambio\")\n",
                "    plt.show()\n",
                "\n",
                "visualize_neuron_dynamics(sanity_model, sanity_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÜ Fase 3: El Gran Torneo (Ahora con Confianza)\n",
                "Ya sabemos que la arquitectura funciona. Ahora s√≠, lancemos la b√∫squeda masiva."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Descargar Datos Reales (Shakespeare)\n",
                "def get_real_data():\n",
                "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
                "    return requests.get(url).text\n",
                "\n",
                "raw_text = get_real_data()\n",
                "data_tensor = torch.tensor([ord(c) for c in raw_text], dtype=torch.long)\n",
                "train_data = data_tensor[:int(0.9*len(data_tensor))]\n",
                "val_data = data_tensor[int(0.9*len(data_tensor)):]\n",
                "\n",
                "def get_batch(split='train'):\n",
                "    data = train_data if split == 'train' else val_data\n",
                "    ix = torch.randint(len(data) - 64, (32,))\n",
                "    x = torch.stack([data[i:i+64] for i in ix]).to(device)\n",
                "    y = torch.stack([data[i+1:i+65] for i in ix]).to(device)\n",
                "    return x, y\n",
                "\n",
                "# Torneo R√°pido (Demo)\n",
                "def run_tournament_demo():\n",
                "    print(\"üî• Iniciando Torneo...\")\n",
                "    results = []\n",
                "    for i in range(5): # Solo 5 para demo r√°pida\n",
                "        config = {\n",
                "            'n_layers': random.choice([2, 4]),\n",
                "            'd_model': random.choice([128, 256]),\n",
                "            'n_heads': 4,\n",
                "            'learning_rate': 1e-3\n",
                "        }\n",
                "        model = CortexOrganism(config).to(device)\n",
                "        optim = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
                "        \n",
                "        # Entrenar 20 iteraciones\n",
                "        losses = []\n",
                "        for _ in range(20):\n",
                "            xb, yb = get_batch()\n",
                "            _, loss = model(xb, yb)\n",
                "            optim.zero_grad()\n",
                "            loss.backward()\n",
                "            optim.step()\n",
                "            losses.append(loss.item())\n",
                "            \n",
                "        print(f\"   Modelo {i} {config}: Loss {losses[-1]:.4f}\")\n",
                "        results.append({'id': i, 'loss': losses[-1], **config})\n",
                "        \n",
                "    df = pd.DataFrame(results)\n",
                "    display(df.sort_values('loss'))\n",
                "\n",
                "run_tournament_demo()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}