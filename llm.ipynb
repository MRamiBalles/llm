{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Cortex-1 Research Lab\n",
    "\n",
    "Bienvenido al entorno de desarrollo de **Cortex-1**, una arquitectura h√≠brida dise√±ada para superar a los Transformers tradicionales.\n",
    "\n",
    "## üß¨ La Arquitectura: Hybrid Mamba-MoE\n",
    "En lugar de usar solo Atenci√≥n (como GPT), usamos un enfoque biol√≥gico:\n",
    "1.  **Mamba (SSM)**: Act√∫a como el \"Hipocampo\", proporcionando memoria de largo plazo infinita y lineal.\n",
    "2.  **Atenci√≥n**: Act√∫a como la \"Corteza Prefrontal\", razonando sobre la informaci√≥n inmediata.\n",
    "3.  **MoE (Mixture of Experts)**: Regiones especializadas del cerebro que se activan solo cuando es necesario.\n",
    "4.  **Byte-Level**: Sin tokenizador. El modelo lee bytes crudos (0-255), entendiendo el \"ADN\" de la informaci√≥n.\n",
    "\n",
    "### Diagrama del Sistema\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Bytes] --> B(Byte Embedding)\n",
    "    B --> C{Backbone Loop}\n",
    "    C -->|Layer 1, 3...| D[Mamba Block <br> Long-Term Memory]\n",
    "    C -->|Layer 2, 4...| E[Attention Block <br> Reasoning]\n",
    "    D --> C\n",
    "    E --> C\n",
    "    C --> F[MoE Router]\n",
    "    F --> G[Expert 1: Logic]\n",
    "    F --> H[Expert 2: Code]\n",
    "    F --> I[Expert 3: Arts]\n",
    "    G & H & I --> J(Layer Norm)\n",
    "    J --> K[Output Logits]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuraci√≥n del Entorno\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "# Configuraci√≥n de dispositivo\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üöÄ Cortex-1 Engine running on: {device.upper()}\")\n",
    "\n",
    "# Hiperpar√°metros Gen√©ticos (Genome)\n",
    "config = {\n",
    "    'vocab_size': 256,      # Byte-level\n",
    "    'd_model': 384,         # Dimensi√≥n del embedding\n",
    "    'n_layers': 6,          # Profundidad\n",
    "    'n_experts': 4,         # N√∫mero de expertos MoE\n",
    "    'top_k': 2,             # Expertos activos por token\n",
    "    'block_size': 256,      # Contexto para la parte de atenci√≥n\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 3e-4,\n",
    "    'batch_size': 32,\n",
    "    'max_iters': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Fase 1: Universal Curriculum (Data Scraper)\n",
    "Vamos a descargar papers \"disruptivos\" en tiempo real para entrenar al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. El Recolector de Conocimiento (Scraper)\n",
    "def download_disruptive_papers():\n",
    "    print(\"üì° Escaneando ArXiv en busca de conocimiento disruptivo...\")\n",
    "    # Consulta simplificada para demostraci√≥n\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=all:transformer+AND+all:attention&start=0&max_results=3'\n",
    "    data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    root = ET.fromstring(data)\n",
    "    \n",
    "    texts = []\n",
    "    ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "    \n",
    "    for entry in root.findall('atom:entry', ns):\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        print(f\"   üìÑ Ingestando: {title[:50]}...\")\n",
    "        texts.append(f\"Title: {title}\\nAbstract: {summary}\\n\\n\")\n",
    "    \n",
    "    return \"\".join(texts)\n",
    "\n",
    "# Si no tenemos datos locales, descargamos algo para probar\n",
    "raw_text = download_disruptive_papers()\n",
    "print(f\"\\n‚úÖ Dataset cargado: {len(raw_text)} caracteres.\")\n",
    "\n",
    "# Preprocesamiento Byte-Level (Sin Tokenizer)\n",
    "data_tensor = torch.tensor([b for b in raw_text.encode('utf-8')], dtype=torch.long)\n",
    "n = int(0.9 * len(data_tensor))\n",
    "train_data = data_tensor[:n]\n",
    "val_data = data_tensor[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config['block_size'], (config['batch_size'],))\n",
    "    x = torch.stack([data[i:i+config['block_size']] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config['block_size']+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Fase 2: Implementaci√≥n de Cortex-1\n",
    "Aqu√≠ reside la magia: **Mamba Block** (memoria) + **MoE Layer** (especializaci√≥n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Componentes de la Arquitectura\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\" Implementaci√≥n simplificada de State Space Model para demostraci√≥n \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        dim = config['d_model']\n",
    "        self.in_proj = nn.Linear(dim, dim * 2)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.dt_proj = nn.Linear(dim, dim)\n",
    "        # En una implementaci√≥n real, esto usa CUDA kernels optimizados\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Simulaci√≥n funcional del paso selectivo\n",
    "        B, L, D = x.shape\n",
    "        x_and_res = self.in_proj(x)\n",
    "        x_val, res = x_and_res.chunk(2, dim=-1)\n",
    "        # Bypass simple para demo (el verdadero Mamba requiere compilaci√≥n compleja)\n",
    "        x_val = x_val * F.sigmoid(self.dt_proj(x_val))\n",
    "        return self.out_proj(x_val * F.silu(res))\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" Un experto individual (Feed Forward Network) \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], 4 * config['d_model']),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config['d_model'], config['d_model']),\n",
    "            nn.Dropout(config['dropout'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\" Capa Mixture of Experts con Gating Top-K \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config['n_experts']\n",
    "        self.top_k = config['top_k']\n",
    "        self.experts = nn.ModuleList([Expert(config) for _ in range(self.num_experts)])\n",
    "        self.gate = nn.Linear(config['d_model'], self.num_experts)\n",
    "        self.last_gate_logits = None # Para visualizaci√≥n\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C)\n",
    "        gate_logits = self.gate(x_flat)\n",
    "        self.last_gate_logits = gate_logits.detach() # Guardamos para graficar\n",
    "        \n",
    "        weights, indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        results = torch.zeros_like(x_flat)\n",
    "        # Ejecuci√≥n naive (lenta) para demo. En prod usar scatter/gather.\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = (indices == i).any(dim=-1)\n",
    "            if mask.any():\n",
    "                expert_out = expert(x_flat[mask])\n",
    "                # Simplificaci√≥n de la suma ponderada para mantener el c√≥digo corto\n",
    "                results[mask] += expert_out \n",
    "                \n",
    "        return results.view(B, T, C)\n",
    "\n",
    "class CortexHybrid(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(config['n_layers']):\n",
    "            # Alternamos Mamba (Memoria) y Atenci√≥n (Razonamiento)\n",
    "            if i % 2 == 0:\n",
    "                self.layers.append(MambaBlock(config))\n",
    "            else:\n",
    "                self.layers.append(nn.TransformerEncoderLayer(\n",
    "                    d_model=config['d_model'], nhead=4, dim_feedforward=4*config['d_model'], \n",
    "                    dropout=config['dropout'], batch_first=True\n",
    "                ))\n",
    "        \n",
    "        self.moe = MoELayer(config)\n",
    "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
    "        self.head = nn.Linear(config['d_model'], config['vocab_size'])\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.embedding(idx)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.moe(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "model = CortexHybrid(config).to(device)\n",
    "print(f\"üß† Cortex-1 Inicializado. Par√°metros: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Fase 3: Entrenamiento y Visualizaci√≥n en Tiempo Real\n",
    "Observa c√≥mo el modelo aprende y c√≥mo decide qu√© expertos usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Bucle de Entrenamiento con Dashboard\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "train_losses = []\n",
    "expert_usage = torch.zeros(config['n_experts'])\n",
    "\n",
    "plt.ion()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "print(\"üî• Iniciando ignici√≥n de Cortex-1...\")\n",
    "\n",
    "for iter in range(config['max_iters']):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Capturar uso de expertos (simulado desde logits del gate)\n",
    "    if model.moe.last_gate_logits is not None:\n",
    "        # Sumar activaciones crudas para ver preferencias\n",
    "        usage = model.moe.last_gate_logits.mean(dim=0).cpu().detach()\n",
    "        expert_usage = 0.9 * expert_usage + 0.1 * usage # Promedio m√≥vil\n",
    "\n",
    "    if iter % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Gr√°fica 1: P√©rdida (Aprendizaje)\n",
    "        ax1.clear()\n",
    "        ax1.plot(train_losses, label='Training Loss', color='#38bdf8')\n",
    "        ax1.set_title('Curva de Aprendizaje (Loss)')\n",
    "        ax1.set_xlabel('Iteraciones')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.grid(True, alpha=0.1)\n",
    "        \n",
    "        # Gr√°fica 2: Distribuci√≥n de Expertos MoE\n",
    "        ax2.clear()\n",
    "        ax2.bar(range(config['n_experts']), expert_usage, color='#818cf8')\n",
    "        ax2.set_title('Activaci√≥n de Expertos (MoE)')\n",
    "        ax2.set_xlabel('ID del Experto')\n",
    "        ax2.set_ylabel('Nivel de Actividad')\n",
    "        \n",
    "        display(fig)\n",
    "        print(f\"Iter {iter}: Loss {loss.item():.4f}\")\n",
    "\n",
    "plt.ioff()\n",
    "print(\"‚úÖ Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prueba de Generaci√≥n (Inferencia)\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    # Convertir texto a bytes\n",
    "    idx = torch.tensor([b for b in prompt.encode('utf-8')], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -config['block_size']:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    # Decodificar bytes a texto (ignorando errores de utf-8 parciales)\n",
    "    return bytes(idx[0].tolist()).decode('utf-8', errors='replace')\n",
    "\n",
    "print(\"ü§ñ Cortex-1 dice:\")\n",
    "print(generate(\"Artificial Intelligence is\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}