{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ Cortex-3: Battle Arena & Systematic Search\n",
                "\n",
                "## üö® Diagn√≥stico de Errores\n",
                "El usuario report√≥ salida \"basura\" (``). Esto ocurre en modelos **Byte-Level** cuando:\n",
                "1.  **Falta de Convergencia**: El modelo no ha aprendido las reglas b√°sicas de UTF-8 (que ciertos bytes siempre van juntos).\n",
                "2.  **Temperatura Alta**: El muestreo es demasiado ca√≥tico.\n",
                "3.  **Arquitectura Inestable**: Mamba o MoE pueden tener gradientes que explotan.\n",
                "\n",
                "## ‚öîÔ∏è La Soluci√≥n: Battle Arena\n",
                "En lugar de confiar en una evoluci√≥n ciega, vamos a enfrentar a las arquitecturas en igualdad de condiciones.\n",
                "Entrenaremos 3 modelos simult√°neamente:\n",
                "1.  üîµ **Transformer Puro** (Baseline)\n",
                "2.  üü¢ **Mamba Puro** (SSM)\n",
                "3.  üî¥ **Cortex Hybrid** (Nuestra apuesta)\n",
                "\n",
                "Veremos cu√°l converge m√°s r√°pido y cu√°l genera texto limpio.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 0. Configuraci√≥n Robusta\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import random\n",
                "from IPython.display import clear_output, display\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"üöÄ Cortex-3 Engine: {device.upper()}\")\n",
                "\n",
                "def set_seed(seed=42):\n",
                "    random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Componentes (Mismos de antes, asegurando estabilidad) ---\n",
                "\n",
                "class MambaBlock(nn.Module):\n",
                "    def __init__(self, d_model):\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_model, d_model * 2)\n",
                "        self.out_proj = nn.Linear(d_model, d_model)\n",
                "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        B, L, D = x.shape\n",
                "        x_and_res = self.in_proj(x)\n",
                "        x_val, res = x_and_res.chunk(2, dim=-1)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = self.conv(x_val)\n",
                "        x_val = x_val.transpose(1, 2)\n",
                "        x_val = F.silu(x_val)\n",
                "        return self.out_proj(x_val * F.sigmoid(res))\n",
                "\n",
                "class CortexOrganism(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        self.embedding = nn.Embedding(256, config['d_model'])\n",
                "        self.layers = nn.ModuleList()\n",
                "        \n",
                "        for i in range(config['n_layers']):\n",
                "            if config['backbone'] == 'mamba':\n",
                "                self.layers.append(MambaBlock(config['d_model']))\n",
                "            elif config['backbone'] == 'transformer':\n",
                "                self.layers.append(nn.TransformerEncoderLayer(\n",
                "                    d_model=config['d_model'], nhead=config['n_heads'], \n",
                "                    dim_feedforward=4*config['d_model'], batch_first=True\n",
                "                ))\n",
                "            elif config['backbone'] == 'hybrid':\n",
                "                if i % 2 == 0: self.layers.append(MambaBlock(config['d_model']))\n",
                "                else: self.layers.append(nn.TransformerEncoderLayer(\n",
                "                    d_model=config['d_model'], nhead=config['n_heads'], \n",
                "                    dim_feedforward=4*config['d_model'], batch_first=True\n",
                "                ))\n",
                "        \n",
                "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
                "        self.head = nn.Linear(config['d_model'], 256)\n",
                "\n",
                "    def forward(self, idx, targets=None):\n",
                "        x = self.embedding(idx)\n",
                "        for layer in self.layers:\n",
                "            x = layer(x)\n",
                "        x = self.ln_f(x)\n",
                "        logits = self.head(x)\n",
                "        \n",
                "        loss = None\n",
                "        if targets is not None:\n",
                "            B, T, C = logits.shape\n",
                "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
                "        return logits, loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öîÔ∏è Battle Arena: Comparativa Directa\n",
                "Entrenamos los 3 modelos a la vez con los mismos datos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Datos Dummy (Reemplazar con Scraper si hay internet)\n",
                "dummy_data = torch.randint(0, 256, (5000,), dtype=torch.long)\n",
                "def get_batch():\n",
                "    ix = torch.randint(len(dummy_data) - 64, (32,))\n",
                "    x = torch.stack([dummy_data[i:i+64] for i in ix]).to(device)\n",
                "    y = torch.stack([dummy_data[i+1:i+65] for i in ix]).to(device)\n",
                "    return x, y\n",
                "\n",
                "# Configuraciones de los Contendientes\n",
                "base_config = {'n_layers': 4, 'd_model': 256, 'n_heads': 4, 'learning_rate': 1e-3}\n",
                "\n",
                "models = {\n",
                "    'Transformer': CortexOrganism({**base_config, 'backbone': 'transformer'}).to(device),\n",
                "    'Mamba': CortexOrganism({**base_config, 'backbone': 'mamba'}).to(device),\n",
                "    'Hybrid': CortexOrganism({**base_config, 'backbone': 'hybrid'}).to(device)\n",
                "}\n",
                "\n",
                "optimizers = {name: torch.optim.AdamW(m.parameters(), lr=1e-3) for name, m in models.items()}\n",
                "history = {name: [] for name in models}\n",
                "\n",
                "print(\"üîî ¬°Que comience la batalla!\")\n",
                "\n",
                "for step in range(500):\n",
                "    xb, yb = get_batch()\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        _, loss = model(xb, yb)\n",
                "        \n",
                "        optimizers[name].zero_grad()\n",
                "        loss.backward()\n",
                "        # Gradient Clipping para evitar explosiones (Crucial para Mamba)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizers[name].step()\n",
                "        \n",
                "        history[name].append(loss.item())\n",
                "    \n",
                "    if step % 50 == 0:\n",
                "        clear_output(wait=True)\n",
                "        plt.figure(figsize=(10, 6))\n",
                "        for name, losses in history.items():\n",
                "            plt.plot(losses, label=name)\n",
                "        plt.title(\"Battle Arena: Loss Comparison\")\n",
                "        plt.xlabel(\"Iteraciones\")\n",
                "        plt.ylabel(\"Loss\")\n",
                "        plt.legend()\n",
                "        plt.grid(True, alpha=0.3)\n",
                "        plt.show()\n",
                "        print(f\"Step {step}: T={history['Transformer'][-1]:.3f}, M={history['Mamba'][-1]:.3f}, H={history['Hybrid'][-1]:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîç B√∫squeda Sistem√°tica (Grid Search)\n",
                "En lugar de azar, probamos combinaciones espec√≠ficas para encontrar estabilidad."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grid_search():\n",
                "    # Espacio de b√∫squeda reducido y sensato\n",
                "    layers_options = [2, 4]\n",
                "    dim_options = [128, 256]\n",
                "    \n",
                "    results = []\n",
                "    \n",
                "    print(\"üîç Iniciando Grid Search...\")\n",
                "    for n_layers in layers_options:\n",
                "        for d_model in dim_options:\n",
                "            config = {'n_layers': n_layers, 'd_model': d_model, 'n_heads': 4, 'backbone': 'hybrid'}\n",
                "            model = CortexOrganism(config).to(device)\n",
                "            optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
                "            \n",
                "            # Sprint corto\n",
                "            losses = []\n",
                "            for _ in range(50):\n",
                "                xb, yb = get_batch()\n",
                "                _, loss = model(xb, yb)\n",
                "                optim.zero_grad()\n",
                "                loss.backward()\n",
                "                optim.step()\n",
                "                losses.append(loss.item())\n",
                "            \n",
                "            final_loss = sum(losses[-10:]) / 10\n",
                "            results.append({'layers': n_layers, 'dim': d_model, 'loss': final_loss})\n",
                "            print(f\"   Config [L={n_layers}, D={d_model}] -> Loss: {final_loss:.4f}\")\n",
                "    \n",
                "    # Mostrar mapa de calor de resultados\n",
                "    df = pd.DataFrame(results)\n",
                "    pivot = df.pivot(index='layers', columns='dim', values='loss')\n",
                "    plt.figure(figsize=(6, 4))\n",
                "    sns.heatmap(pivot, annot=True, cmap='viridis_r') # Invertido: Azul es mejor (menor loss)\n",
                "    plt.title(\"Grid Search Results (Loss)\")\n",
                "    plt.show()\n",
                "\n",
                "grid_search()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ†Ô∏è Generaci√≥n Robusta (Fixing the Garbage Output)\n",
                "Aqu√≠ solucionamos el problema de los caracteres extra√±os (``)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def safe_generate(model, prompt, max_len=100, temperature=0.7):\n",
                "    model.eval()\n",
                "    idx = torch.tensor([b for b in prompt.encode('utf-8')], dtype=torch.long).unsqueeze(0).to(device)\n",
                "    \n",
                "    for _ in range(max_len):\n",
                "        with torch.no_grad():\n",
                "            logits, _ = model(idx)\n",
                "            logits = logits[:, -1, :] / temperature # Controlar caos\n",
                "            probs = F.softmax(logits, dim=-1)\n",
                "            \n",
                "            # Sampling m√°s conservador (Top-K)\n",
                "            top_k = 10\n",
                "            v, _ = torch.topk(probs, top_k)\n",
                "            probs[probs < v[:, [-1]]] = 0\n",
                "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
                "            \n",
                "            next_token = torch.multinomial(probs, 1)\n",
                "            idx = torch.cat((idx, next_token), dim=1)\n",
                "            \n",
                "    # Decodificaci√≥n resiliente\n",
                "    raw_bytes = idx[0].tolist()\n",
                "    decoded = bytes(raw_bytes).decode('utf-8', errors='replace')\n",
                "    return decoded\n",
                "\n",
                "print(\"ü§ñ Generaci√≥n (Transformer):\", safe_generate(models['Transformer'], \"AI is\"))\n",
                "print(\"ü§ñ Generaci√≥n (Hybrid):\", safe_generate(models['Hybrid'], \"AI is\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}