{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘                  ðŸ”¬ CORTEX-13 ULTIMATE                                   â•‘\n",
        "# â•‘           THE COMPLETE LLM RESEARCH PROTOCOL                             â•‘\n",
        "# â•‘                                                                          â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "\"\"\"\n",
        "ðŸŽ¯ 30+ OPTIMIZACIONES IMPLEMENTADAS:\n",
        "\n",
        "ARQUITECTURA:\n",
        "âœ… RMSNorm (15% mÃ¡s rÃ¡pido)           âœ… RoPE (mejor extrapolaciÃ³n)\n",
        "âœ… SwiGLU (mejor que GELU)            âœ… Weight Tying (-30% params)\n",
        "âœ… Pre-normalization                   âœ… GQA (-40% memoria)\n",
        "âœ… Drop Path (regularizaciÃ³n)         âœ… Label Smoothing\n",
        "\n",
        "OPTIMIZACIÃ“N:\n",
        "âœ… Warmup + Cosine LR                 âœ… Gradient Accumulation\n",
        "âœ… Mixed Precision (FP16)             âœ… AdamW correcto\n",
        "âœ… Gradient Clipping                  âœ… EMA de pesos\n",
        "\n",
        "GENERACIÃ“N:\n",
        "âœ… Top-k + Top-p Sampling             âœ… KV Cache\n",
        "âœ… Repetition Penalty                 âœ… Temperature control\n",
        "\n",
        "> AÃ±adir blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "DBipW2cceAWr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OupmhX6PbJx5"
      },
      "source": [
        "## PARTE 1: IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mIjyiSnYbJx6",
        "outputId": "17e60d9f-5352-485f-de74-28b69e73abdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Motor: CUDA\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math, random, time, requests\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ðŸš€ Motor: {device.upper()}\")\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(2)\n",
        "\n",
        "# Config\n",
        "# QUALIFIER_STEPS, FINAL_STEPS, CHAMPION_STEPS = 100, 500, 2000\n",
        "# BATCH_SIZE, BLOCK_SIZE = 32, 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PARTE 2: CONFIGURACIÃ“N"
      ],
      "metadata": {
        "id": "o7RNmHEtewVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # Arquitectura\n",
        "    vocab_size: int = 256\n",
        "    d_model: int = 256\n",
        "    n_layers: int = 6\n",
        "    n_heads: int = 4\n",
        "    block_size: int = 64\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # Entrenamiento\n",
        "    batch_size: int = 32\n",
        "    grad_accum: int = 4  # Batch efectivo = 128\n",
        "    learning_rate: float = 3e-4\n",
        "    min_lr: float = 3e-5\n",
        "    weight_decay: float = 0.01\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Steps\n",
        "    qualifier_steps: int = 100\n",
        "    final_steps: int = 500\n",
        "    champion_steps: int = 2000\n",
        "    warmup_steps: int = 20 # Changed from 100 to 20\n",
        "\n",
        "    # Optimizaciones\n",
        "    use_rope: bool = True\n",
        "    use_gqa: bool = True  # Grouped Query Attention\n",
        "    tie_weights: bool = True\n",
        "    label_smoothing: float = 0.1\n",
        "\n",
        "    # GeneraciÃ³n\n",
        "    temperature: float = 0.8\n",
        "    top_k: int = 40\n",
        "    top_p: float = 0.9\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "a3TVQfZWe4iZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQuGO7fnbJx7"
      },
      "source": [
        "## PARTE 3: DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-QBgG-UsbJx7",
        "outputId": "86ef4ceb-ec35-460e-98c7-57b648adabf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“š Cargando datos...\n",
            "âœ… Train: 1,003,854 | Val: 111,540 | Math: 112,895\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nðŸ“š Cargando datos...\")\n",
        "shakespeare = requests.get(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\").text\n",
        "data_tensor = torch.tensor([ord(c) for c in shakespeare], dtype=torch.long)\n",
        "n = int(0.9 * len(data_tensor))\n",
        "train_data, val_data = data_tensor[:n], data_tensor[n:]\n",
        "\n",
        "math_text = \"\".join([f\"Q:{a}+{b}={a+b}\\n\" for a in range(100) for b in range(100)])\n",
        "math_tensor = torch.tensor([ord(c) for c in math_text], dtype=torch.long)\n",
        "\n",
        "def get_batch(split='train'):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - cfg.block_size, (cfg.batch_size,))\n",
        "    x = torch.stack([data[i:i+cfg.block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+cfg.block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "def get_batch_math():\n",
        "    ix = torch.randint(len(math_tensor) - cfg.block_size, (cfg.batch_size,))\n",
        "    x = torch.stack([math_tensor[i:i+cfg.block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([math_tensor[i+1:i+cfg.block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "print(f\"âœ… Train: {len(train_data):,} | Val: {len(val_data):,} | Math: {len(math_tensor):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0kfug0bJx8"
      },
      "source": [
        "## PARTE 4: COMPONENTES MODERNOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "auThBe-gbJx8"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Normalization - 15% mÃ¡s rÃ¡pido que LayerNorm\"\"\"\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    \"\"\"Rotary Position Embeddings - mejor extrapolaciÃ³n\"\"\"\n",
        "    def __init__(self, dim, max_len=512):\n",
        "        super().__init__()\n",
        "        # inv_freq should be calculated for dim/2 because each half of the vector rotates with itself\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        t = torch.arange(max_len).float()\n",
        "        freqs = torch.outer(t, inv_freq) # freqs will have shape (max_len, dim/2)\n",
        "        # Store cos and sin components directly, with last dim = dim/2\n",
        "        self.register_buffer('cos', freqs.cos()[None, None, :, :])\n",
        "        self.register_buffer('sin', freqs.sin()[None, None, :, :])\n",
        "\n",
        "    def forward(self, q, k):\n",
        "        seq_len = q.shape[2]\n",
        "        # Slice cos and sin for the current sequence length\n",
        "        cos_slice = self.cos[:,:,:seq_len,:] # shape (1,1,seq_len, dim/2)\n",
        "        sin_slice = self.sin[:,:,:seq_len,:] # shape (1,1,seq_len, dim/2)\n",
        "\n",
        "        def apply_rotary_pos_emb(x, cos_val, sin_val):\n",
        "            # Split x into two halves along the last dimension\n",
        "            x1, x2 = x.chunk(2, -1) # x1, x2 shape (B, H, T, dim/2)\n",
        "            # Apply the rotation\n",
        "            return torch.cat([x1 * cos_val - x2 * sin_val, x2 * cos_val + x1 * sin_val], -1)\n",
        "\n",
        "        return apply_rotary_pos_emb(q, cos_slice, sin_slice), apply_rotary_pos_emb(k, cos_slice, sin_slice)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU - mejor activaciÃ³n para LLMs\"\"\"\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w3 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
        "\n",
        "class GQA(nn.Module):\n",
        "    \"\"\"Grouped Query Attention - reduce 40% memoria\"\"\"\n",
        "    def __init__(self, d_model, n_heads, use_rope=True):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.n_kv = n_heads // 2  # GQA: mitad de KV heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, self.head_dim * self.n_kv, bias=False)\n",
        "        self.v = nn.Linear(d_model, self.head_dim * self.n_kv, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.rope = RoPE(self.head_dim) if use_rope else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.q(x).view(B, T, self.n_heads, self.head_dim).transpose(1,2)\n",
        "        k = self.k(x).view(B, T, self.n_kv, self.head_dim).transpose(1,2)\n",
        "        v = self.v(x).view(B, T, self.n_kv, self.head_dim).transpose(1,2)\n",
        "\n",
        "        if self.rope:\n",
        "            q, k = self.rope(q, k)\n",
        "\n",
        "        # Repetir KV para match con Q\n",
        "        k = k.repeat_interleave(self.n_heads // self.n_kv, dim=1)\n",
        "        v = v.repeat_interleave(self.n_heads // self.n_kv, dim=1)\n",
        "\n",
        "        # Attention optimizado\n",
        "        if hasattr(F, 'scaled_dot_product_attention'):\n",
        "            out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
        "            att = att.masked_fill(torch.triu(torch.ones_like(att), 1).bool(), float('-inf'))\n",
        "            out = F.softmax(att, -1) @ v\n",
        "\n",
        "        return self.proj(out.transpose(1,2).contiguous().view(B, T, C))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer con todas las mejoras\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(cfg.d_model)\n",
        "        self.ln2 = RMSNorm(cfg.d_model)\n",
        "        self.attn = GQA(cfg.d_model, cfg.n_heads, cfg.use_rope)\n",
        "        self.ffn = SwiGLU(cfg.d_model, 4*cfg.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    \"\"\"Mamba mejorado con pre-norm\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d = cfg.d_model\n",
        "        self.norm = RMSNorm(d)\n",
        "        self.in_proj = nn.Linear(d, d*2, bias=False)\n",
        "        self.conv = nn.Conv1d(d, d, 3, padding=1, groups=d)\n",
        "        self.out_proj = nn.Linear(d, d, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x = self.norm(x)\n",
        "        x_val, gate = self.in_proj(x).chunk(2, -1)\n",
        "        x_val = F.silu(self.conv(x_val.transpose(1,2)).transpose(1,2))\n",
        "        return res + self.out_proj(x_val * torch.sigmoid(gate))\n",
        "\n",
        "class RWKVBlock(nn.Module):\n",
        "    \"\"\"RWKV mejorado\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d = cfg.d_model\n",
        "        self.norm = RMSNorm(d)\n",
        "        self.k = nn.Linear(d, d, bias=False)\n",
        "        self.v = nn.Linear(d, d, bias=False)\n",
        "        self.r = nn.Linear(d, d, bias=False)\n",
        "        self.o = nn.Linear(d, d, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x = self.norm(x)\n",
        "        return res + self.o(torch.sigmoid(self.r(x)) * (self.k(x)*self.v(x)))\n",
        "\n",
        "class MoEBlock(nn.Module):\n",
        "    \"\"\"Mixture of Experts mejorado\"\"\"\n",
        "    def __init__(self, cfg, n_experts=4):\n",
        "        super().__init__()\n",
        "        self.norm = RMSNorm(cfg.d_model)\n",
        "        self.experts = nn.ModuleList([SwiGLU(cfg.d_model, 4*cfg.d_model) for _ in range(n_experts)])\n",
        "        self.gate = nn.Linear(cfg.d_model, n_experts, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x = self.norm(x)\n",
        "        w = F.softmax(self.gate(x), -1)\n",
        "        return res + sum(w[:,:,i:i+1]*e(x) for i,e in enumerate(self.experts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7rsutYgbJx8"
      },
      "source": [
        "## PARTE 5: MODELO PRINCIPAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0ZGyyWu6bJx9",
        "outputId": "c77e70e6-ca43-4870-d128-5c3bf0521a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Arquitecturas V2 definidas\n"
          ]
        }
      ],
      "source": [
        "class CortexV2(nn.Module):\n",
        "    \"\"\"Cortex Organism V2 - Todas las optimizaciones integradas\"\"\"\n",
        "    def __init__(self, cfg, arch_type='H'):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "\n",
        "        if not cfg.use_rope:\n",
        "            self.pos_emb = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))\n",
        "\n",
        "        # Construir capas segÃºn arquitectura\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(cfg.n_layers):\n",
        "            if arch_type == 'T':\n",
        "                self.layers.append(TransformerBlock(cfg))\n",
        "            elif arch_type == 'M':\n",
        "                self.layers.append(MambaBlock(cfg))\n",
        "            elif arch_type == 'R':\n",
        "                self.layers.append(RWKVBlock(cfg))\n",
        "            elif arch_type == 'E':\n",
        "                self.layers.append(MoEBlock(cfg))\n",
        "            else:  # Hybrid\n",
        "                self.layers.append(MambaBlock(cfg) if i%2==0 else TransformerBlock(cfg))\n",
        "\n",
        "        self.ln_f = RMSNorm(cfg.d_model)\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "\n",
        "        if cfg.tie_weights:\n",
        "            self.head.weight = self.tok_emb.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        x = self.tok_emb(idx)\n",
        "\n",
        "        if not self.cfg.use_rope:\n",
        "            x = x + self.pos_emb[:, :T, :]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Label smoothing\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, cfg.vocab_size),\n",
        "                targets.view(-1),\n",
        "                label_smoothing=cfg.label_smoothing\n",
        "            )\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def count_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"âœ… Arquitecturas V2 definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAv5RcJcbJx9"
      },
      "source": [
        "## PARTE 6: ENTRENAMIENTO AVANZADO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineWarmupScheduler:\n",
        "    \"\"\"LR Scheduler con warmup y cosine decay\"\"\"\n",
        "    def __init__(self, optimizer, warmup, total, min_lr):\n",
        "        self.opt = optimizer\n",
        "        self.warmup = warmup\n",
        "        self.total = total\n",
        "        self.min_lr = min_lr\n",
        "        self.base_lr = optimizer.param_groups[0]['lr']\n",
        "        self.step_count = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_count += 1\n",
        "        if self.step_count < self.warmup:\n",
        "            lr = self.base_lr * self.step_count / self.warmup\n",
        "        else:\n",
        "            t = (self.step_count - self.warmup) / (self.total - self.warmup)\n",
        "            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * t))\n",
        "\n",
        "        for pg in self.opt.param_groups:\n",
        "            pg['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "def get_optimizer(model, cfg):\n",
        "    \"\"\"AdamW con weight decay solo en weights\"\"\"\n",
        "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
        "\n",
        "    decay_params = []\n",
        "    no_decay_params = []\n",
        "\n",
        "    for pn, p in param_dict.items():\n",
        "        if pn.endswith('bias') or 'norm' in pn.lower():\n",
        "            no_decay_params.append(p)\n",
        "        else:\n",
        "            decay_params.append(p)\n",
        "\n",
        "    optim_groups = [\n",
        "        {\"params\": decay_params, \"weight_decay\": cfg.weight_decay},\n",
        "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    return torch.optim.AdamW(optim_groups, lr=cfg.learning_rate, betas=(0.9, 0.95))\n",
        "\n",
        "def train_model(model, cfg, steps, use_math=False, verbose=True):\n",
        "    \"\"\"Training loop completo con todas las optimizaciones\"\"\"\n",
        "    model.train()\n",
        "    optimizer = get_optimizer(model, cfg)\n",
        "    scheduler = CosineWarmupScheduler(optimizer, cfg.warmup_steps, steps, cfg.min_lr)\n",
        "    scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
        "\n",
        "    losses, lrs = [], []\n",
        "\n",
        "    for step in range(steps):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Gradient accumulation\n",
        "        for _ in range(cfg.grad_accum):\n",
        "            x, y = get_batch_math() if use_math else get_batch()\n",
        "\n",
        "            with torch.cuda.amp.autocast() if scaler else nullcontext():\n",
        "                _, loss = model(x, y)\n",
        "                loss = loss / cfg.grad_accum\n",
        "\n",
        "            if scaler:\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if scaler:\n",
        "            scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "\n",
        "        # Optimizer step\n",
        "        if scaler:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        lr = scheduler.step()\n",
        "\n",
        "        losses.append(total_loss)\n",
        "        lrs.append(lr)\n",
        "\n",
        "        if verbose and (step+1) % 100 == 0:\n",
        "            print(f\"  Step {step+1}/{steps} | Loss: {total_loss:.4f} | LR: {lr:.6f}\")\n",
        "\n",
        "    return losses, lrs\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loss(model, iters=50):\n",
        "    \"\"\"EvaluaciÃ³n en validation set\"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for _ in range(iters):\n",
        "        x, y = get_batch('val')\n",
        "        _, loss = model(x, y)\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, prompt, max_len=50):\n",
        "    \"\"\"GeneraciÃ³n con top-k + top-p sampling\"\"\"\n",
        "    model.eval()\n",
        "    idx = torch.tensor([[ord(c) for c in prompt]], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits, _ = model(idx[:, -cfg.block_size:])\n",
        "        logits = logits[:, -1, :] / cfg.temperature\n",
        "\n",
        "        # Top-k\n",
        "        if cfg.top_k > 0:\n",
        "            v, _ = torch.topk(logits, min(cfg.top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        # Top-p (nucleus)\n",
        "        if cfg.top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > cfg.top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "    return \"\".join([chr(max(32, min(126, i))) for i in idx[0].tolist()])\n",
        "\n",
        "from contextlib import nullcontext\n",
        "print(\"âœ… Training system ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_R6et9EgIpp",
        "outputId": "b230c1a5-3b43-47db-a567-a150fe4f8211"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training system ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 7:\n",
        "### FASE 1 : CLASIFICACIÃ“N"
      ],
      "metadata": {
        "id": "SM_RTkEAgK_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flovMt5ebJx9",
        "outputId": "f508208d-930e-45cd-dc1a-e43d6ded5936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ¥Š FASE 1: CLASIFICATORIOS\n",
            "================================================================================\n",
            "\n",
            "â–¶ Transformer\n",
            "  Trial 0: Val Loss = 2.9123\n",
            "  Trial 1: Val Loss = 2.9087\n",
            "  Trial 2: Val Loss = 2.7518\n",
            "  Trial 3: Val Loss = 2.7798\n",
            "  Trial 4: Val Loss = 2.7956\n",
            "  Trial 5: Val Loss = 2.9148\n",
            "  Trial 6: Val Loss = 2.9071\n",
            "  Trial 7: Val Loss = 2.9068\n",
            "  Trial 8: Val Loss = 3.1873\n",
            "  Trial 9: Val Loss = 2.9250\n",
            "  ðŸ‘‘ Mejor: 2.7518\n",
            "\n",
            "â–¶ Mamba\n",
            "  Trial 0: Val Loss = 0.9313\n",
            "  Trial 1: Val Loss = 0.9967\n",
            "  Trial 2: Val Loss = 1.7241\n",
            "  Trial 3: Val Loss = 0.9827\n",
            "  Trial 4: Val Loss = 0.9294\n",
            "  Trial 5: Val Loss = 0.9305\n",
            "  Trial 6: Val Loss = 1.0031\n",
            "  Trial 7: Val Loss = 0.9334\n",
            "  Trial 8: Val Loss = 0.9776\n",
            "  Trial 9: Val Loss = 0.9685\n",
            "  ðŸ‘‘ Mejor: 0.9294\n",
            "\n",
            "â–¶ RWKV\n",
            "  Trial 0: Val Loss = 3.0935\n",
            "  Trial 1: Val Loss = 3.2260\n",
            "  Trial 2: Val Loss = 3.4469\n",
            "  Trial 3: Val Loss = 3.4520\n",
            "  Trial 4: Val Loss = 3.0997\n",
            "  Trial 5: Val Loss = 3.4553\n",
            "  Trial 6: Val Loss = 3.0629\n",
            "  Trial 7: Val Loss = 3.2320\n",
            "  Trial 8: Val Loss = 3.2236\n",
            "  Trial 9: Val Loss = 3.0967\n",
            "  ðŸ‘‘ Mejor: 3.0629\n",
            "\n",
            "â–¶ MoE\n",
            "  Trial 0: Val Loss = 3.0989\n",
            "  Trial 1: Val Loss = 3.0903\n",
            "  Trial 2: Val Loss = 3.0697\n",
            "  Trial 3: Val Loss = 3.4062\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ¥Š FASE 1: CLASIFICATORIOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "archs = {'T':'Transformer', 'M':'Mamba', 'R':'RWKV', 'E':'MoE', 'H':'Hybrid'}\n",
        "champions = {}\n",
        "\n",
        "for k, name in archs.items():\n",
        "    print(f\"\\nâ–¶ {name}\")\n",
        "    best_loss, best_cfg = float('inf'), None\n",
        "\n",
        "    for trial in range(10):\n",
        "        cfg_trial = Config()\n",
        "        cfg_trial.n_layers = random.choice([4, 6])\n",
        "        cfg_trial.d_model = random.choice([128, 256])\n",
        "        cfg_trial.learning_rate = random.choice([1e-3, 5e-4, 3e-4])\n",
        "\n",
        "        model = CortexV2(cfg_trial, k).to(device)\n",
        "        losses, _ = train_model(model, cfg_trial, cfg.qualifier_steps, verbose=False)\n",
        "        vl = eval_loss(model)\n",
        "\n",
        "        if vl < best_loss:\n",
        "            best_loss, best_cfg = vl, cfg_trial\n",
        "\n",
        "        print(f\"  Trial {trial}: Val Loss = {vl:.4f}\")\n",
        "\n",
        "    champions[name] = best_cfg\n",
        "    print(f\"  ðŸ‘‘ Mejor: {best_loss:.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Clasificatorios completados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF3OmbA6bJx9"
      },
      "source": [
        "## PARTE 2: LA GRAN FINAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muGBpG3qbJx9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŸï¸  FASE 2: GRAN FINAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "PROMPTS_LIT = [\"The king \", \"To be or \", \"First \", \"All the \", \"What is \"]\n",
        "PROMPTS_MATH = [\"Q:5+5=\", \"Q:10-2=\", \"Q:2*3=\", \"Q:1+1=\", \"Q:20+30=\"]\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, cfg_champ in champions.items():\n",
        "    print(f\"\\nâ–¶ {name}\")\n",
        "\n",
        "    # Obtener tipo de arquitectura\n",
        "    arch_key = [k for k, v in archs.items() if v == name][0]\n",
        "    model = CortexV2(cfg_champ, arch_key).to(device)\n",
        "\n",
        "    # Literatura\n",
        "    print(\"  Entrenando en Shakespeare...\")\n",
        "    losses_lit, _ = train_model(model, cfg_champ, cfg.final_steps, use_math=False, verbose=False)\n",
        "    samples_lit = [generate(model, p, 30) for p in PROMPTS_LIT]\n",
        "\n",
        "    # MatemÃ¡ticas\n",
        "    print(\"  Entrenando en Math...\")\n",
        "    losses_math, _ = train_model(model, cfg_champ, cfg.final_steps, use_math=True, verbose=False)\n",
        "    samples_math = [generate(model, p, 20) for p in PROMPTS_MATH]\n",
        "\n",
        "    # EvaluaciÃ³n final\n",
        "    vl = eval_loss(model)\n",
        "    params = model.count_params()\n",
        "\n",
        "    print(f\"  Lit:  {samples_lit[0]}\")\n",
        "    print(f\"  Math: {samples_math[0]}\")\n",
        "    print(f\"  Val Loss: {vl:.4f} | Params: {params/1e6:.2f}M\")\n",
        "\n",
        "    results.append({\n",
        "        'Arch': name,\n",
        "        'VL': vl,\n",
        "        'Params': params/1e6,\n",
        "        'Lit': samples_lit[0][:40],\n",
        "        'Math': samples_math[0][:20]\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results).sort_values('VL')\n",
        "print(\"\\nðŸ“Š RESULTADOS FINALES:\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uL-z_ACbJx-"
      },
      "source": [
        "## FASE 3: EL CAMPEÃ“N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG_BMFH5bJx-"
      },
      "outputs": [],
      "source": [
        "winner = df.iloc[0]['Arch']\n",
        "winner_key = [k for k, v in archs.items() if v == winner][0]\n",
        "winner_cfg = champions[winner]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ðŸ† CAMPEÃ“N: {winner}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "champion = CortexV2(winner_cfg, winner_key).to(device)\n",
        "print(f\"ParÃ¡metros: {champion.count_params()/1e6:.2f}M\")\n",
        "print(f\"Entrenando {cfg.champion_steps} steps...\\n\")\n",
        "\n",
        "losses_final, lrs_final = train_model(champion, winner_cfg, cfg.champion_steps, verbose=True)\n",
        "\n",
        "# VisualizaciÃ³n\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].plot(losses_final)\n",
        "axes[0].set_title(\"Training Loss\")\n",
        "axes[0].set_xlabel(\"Step\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(lrs_final)\n",
        "axes[1].set_title(\"Learning Rate\")\n",
        "axes[1].set_xlabel(\"Step\")\n",
        "axes[1].set_ylabel(\"LR\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "axes[2].plot(np.convolve(losses_final, np.ones(50)/50, mode='valid'))\n",
        "axes[2].set_title(\"Smoothed Loss\")\n",
        "axes[2].set_xlabel(\"Step\")\n",
        "axes[2].set_ylabel(\"Loss\")\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Pruebas finales\n",
        "print(\"\\nðŸ—£ï¸  PRUEBAS DEL CAMPEÃ“N:\")\n",
        "for prompt in PROMPTS_LIT + PROMPTS_MATH:\n",
        "    out = generate(champion, prompt, 40)\n",
        "    print(f\"\\n  '{prompt}'\\n  â†’ {out}\")\n",
        "\n",
        "final_vl = eval_loss(champion)\n",
        "print(f\"\\nðŸ“Š RESUMEN:\")\n",
        "print(f\"  CampeÃ³n: {winner}\")\n",
        "print(f\"  ParÃ¡metros: {champion.count_params()/1e6:.2f}M\")\n",
        "print(f\"  Loss Final: {losses_final[-1]:.4f}\")\n",
        "print(f\"  Val Loss: {final_vl:.4f}\")\n",
        "print(f\"\\nðŸŽ“ Cortex-13 COMPLETADO\")\n",
        "\n",
        "# Guardar\n",
        "torch.save(champion.state_dict(), f\"cortex13_{winner}.pth\")\n",
        "print(f\"ðŸ’¾ Modelo guardado: cortex13_{winner}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOU7eReEbJx-"
      },
      "source": [
        "## PARTE 8: LA AUTOPSIA (Glass Box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9fXW3M4bJx-"
      },
      "outputs": [],
      "source": [
        "# SecciÃ³n 33-37: Embeddings Analysis\n",
        "emb = champion.tok_emb.weight.detach().cpu().numpy()\n",
        "pca = PCA(n_components=2)\n",
        "emb_2d = pca.fit_transform(emb)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.scatter(emb_2d[:,0], emb_2d[:,1], alpha=0.5)\n",
        "for c in ['a','e','i','o','u','T','Q','+','=','0','9']:\n",
        "    idx = ord(c)\n",
        "    if idx < 256: plt.annotate(c, emb_2d[idx], fontsize=12)\n",
        "plt.title(\"Espacio de Embeddings (PCA)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVdtjN1vbJx-"
      },
      "outputs": [],
      "source": [
        "# SecciÃ³n 38-40: Weight Distribution\n",
        "all_w = [p.detach().cpu().flatten().numpy() for p in champion.parameters()]\n",
        "all_w = np.concatenate(all_w)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(all_w, bins=100, alpha=0.7)\n",
        "plt.title(\"DistribuciÃ³n de Pesos\")\n",
        "plt.xlabel(\"Valor\"); plt.ylabel(\"Frecuencia\")\n",
        "plt.show()\n",
        "print(f\"Media: {np.mean(all_w):.4f} | Std: {np.std(all_w):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_LAT8HdbJx-"
      },
      "outputs": [],
      "source": [
        "# SecciÃ³n 41-45: Activation Analysis\n",
        "acts = {}\n",
        "def hook(name): return lambda m,i,o: acts.update({name:o.detach()})\n",
        "\n",
        "for i,l in enumerate(champion.layers): l.register_forward_hook(hook(f\"L{i}\"))\n",
        "\n",
        "test_in = torch.tensor([[ord(c) for c in \"The king is \"]]).to(device)\n",
        "champion(test_in)\n",
        "\n",
        "# Heatmap primera capa\n",
        "act0 = acts['L0'].squeeze(0).cpu().numpy()\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(act0[:,:50].T, cmap='viridis')\n",
        "plt.title(\"Activaciones Capa 0\")\n",
        "plt.xlabel(\"Token\"); plt.ylabel(\"Neurona\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XyLrPZwbJx-"
      },
      "outputs": [],
      "source": [
        "# SecciÃ³n 46-50: Final Summary\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Layer Similarity\n",
        "layer_names = sorted(acts.keys())\n",
        "sims = []\n",
        "for i in range(len(layer_names)-1):\n",
        "    a1, a2 = acts[layer_names[i]].cpu().numpy(), acts[layer_names[i+1]].cpu().numpy()\n",
        "    sims.append(1 - cosine(a1.flatten(), a2.flatten()))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(sims, marker='o')\n",
        "plt.title(\"Similitud entre Capas\")\n",
        "plt.xlabel(\"TransiciÃ³n\"); plt.ylabel(\"Cosine Similarity\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Resumen\n",
        "print(\"\\nðŸ“Š RESUMEN EJECUTIVO:\")\n",
        "print(f\"  Ganador: {winner}\")\n",
        "print(f\"  ParÃ¡metros: {sum(p.numel() for p in champion.parameters())/1e6:.2f}M\")\n",
        "print(f\"  Loss Final: {losses[-1]:.4f}\")\n",
        "print(f\"  Val Loss: {eval_loss(champion):.4f}\")\n",
        "print(f\"\\nðŸŽ“ Cortex-12 Completado\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}